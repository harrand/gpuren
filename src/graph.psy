graph : graph_state mut := zero;
graph_state ::= struct
{
	data : graph_data_t mut?;
	count : u64 := 0;
	cap : u64 := 0;
};

impl_gpu_create_graph ::= func(name : u8?, a : arena mut? -> gpu_graph)
{
	ret ::= graph.count;
	graphptr ::= impl_alloc_new_graph(a);
	graphptr->pass_record = zero;
	namelen ::= zstrlen(name);
	graphptr->name = arena_alloc(a, namelen + 1);
	memcopy(graphptr->name, name, namelen);
	deref((graphptr->name) # namelen) = 0;

	initial_timeline_capacity ::= 8;
	graphptr->timeline = arena_alloc(a, __sizeof(graph_entry) * initial_timeline_capacity);
	graphptr->timeline_count = 0;
	graphptr->timeline_cap = initial_timeline_capacity;
	graphptr->dependencies = arena_alloc(a, __sizeof(u64 mut?) * initial_timeline_capacity);
	graphptr->dependency_counts = arena_alloc(a, __sizeof(u64) * initial_timeline_capacity);

	graphptr->event = impl_create_empty_event();
	return ret@gpu_graph;
};

impl_gpu_graph_add_subgraph ::= func(g : gpu_graph, subgraph : gpu_graph, a : arena mut? -> u64)
{
	graphptr ::= graph.data # (g@s64);
	if((graphptr->timeline_count) >= (graphptr->timeline_cap))
	{
		olddata ::= graphptr->timeline;
		oldcap ::= graphptr->timeline_cap;
		graphptr->timeline_cap = (oldcap * 2);
		graphptr->timeline = arena_alloc(a, __sizeof(graph_entry) * (graphptr->timeline_cap));
		memcopy(graphptr->timeline, olddata, __sizeof(graph_entry) * oldcap);

		olddepdata ::= graphptr->dependencies;
		olddepcountdata ::= graphptr->dependency_counts;
		// remember we do indeed use oldcap here, not dependencies_count
		graphptr->dependencies = arena_alloc(a, __sizeof(u64 mut?) * (graphptr->timeline_cap));
		memcopy(graphptr->dependencies, olddepdata, __sizeof(u64 mut?) * oldcap);
		graphptr->dependency_counts = arena_alloc(a, __sizeof(u64) * (graphptr->timeline_cap));
		memcopy(graphptr->dependency_counts, olddepcountdata, __sizeof(u64) * oldcap);
	}
	cur ::= graphptr->timeline_count;
	entry_ptr ::= (graphptr->timeline) # cur;
	(entry_ptr->handle) = (subgraph@s64@_);
	entry_ptr->is_graph = true;
	(graphptr->timeline_count) = (graphptr->timeline_count) + 1;
	
	// todo: dependencies for subgraphs? probably super cursed and hard
	deref(graphptr->dependencies # cur) = zero;
	deref(graphptr->dependency_counts # cur) = zero;
	return cur;
};

impl_gpu_graph_add_pass ::= func(g : gpu_graph, p : gpu_pass, dependencies : u64?, dependencies_count : u64, a : arena mut? -> u64)
{
	graphptr ::= graph.data # (g@s64);
	if((graphptr->timeline_count) >= (graphptr->timeline_cap))
	{
		olddata ::= graphptr->timeline;
		oldcap ::= graphptr->timeline_cap;
		graphptr->timeline_cap = (oldcap * 2);
		graphptr->timeline = arena_alloc(a, __sizeof(graph_entry) * (graphptr->timeline_cap));
		memcopy(graphptr->timeline, olddata, __sizeof(graph_entry) * oldcap);

		olddepdata ::= graphptr->dependencies;
		olddepcountdata ::= graphptr->dependency_counts;
		// remember we do indeed use oldcap here, not dependencies_count
		graphptr->dependencies = arena_alloc(a, __sizeof(u64 mut?) * (graphptr->timeline_cap));
		memcopy(graphptr->dependencies, olddepdata, __sizeof(u64 mut?) * oldcap);
		graphptr->dependency_counts = arena_alloc(a, __sizeof(u64) * (graphptr->timeline_cap));
		memcopy(graphptr->dependency_counts, olddepcountdata, __sizeof(u64) * oldcap);
	}
	cur ::= graphptr->timeline_count;
	entry_ptr ::= (graphptr->timeline) # cur;
	(entry_ptr->handle) = (p@s64@_);
	entry_ptr->is_graph = false;
	(graphptr->timeline_count) = (graphptr->timeline_count) + 1;

	deref(graphptr->dependencies # cur) = arena_alloc(a, __sizeof(u64) * dependencies_count);
	i : u64 mut;
	for(i = 0, i < dependencies_count, i = i + 1)
	{
		deref(deref(graphptr->dependencies # cur) # i) = deref(dependencies # i);
	}
	
	deref(graphptr->dependency_counts # cur) = dependencies_count;
	return cur;
};

impl_gpu_execute ::= func(g : gpu_graph, long : arena mut?, short : arena mut? -> gpu_err)
{
	ret : gpu_err mut := zero;
	graphptr ::= graph.data # (g@s64);
	impl_pass_gpu_clear_views(ref(graphptr->pass_record));
	vk.reset_event(hardware.used_device, graphptr->event);
	writes_to_system_image ::= impl_graph_writes_to_system_image(g);
	will_present ::= impl_graph_will_present(g);

	frame ::= deref(core.frames # (core.current_frame));

	image_index : u32 mut := -1@u32;
	swapchain_image : u64 mut := 0;
	if(will_present)
	{
		// make sure swapchain is available.
		impl_require_swapchain(long);
		if(swapchain.width == 0)
		{
			return zero;
		}
		if(swapchain.height == 0)
		{
			return zero;
		}

		vk_check(vk.acquire_next_image_khr(hardware.used_device, swapchain.handle, 9999999999, 0, frame.swapchain_fence, ref image_index));
		vk_check(vk.wait_for_fences(hardware.used_device, 1, ref (frame.swapchain_fence), 1, ~0));
		vk_check(vk.reset_fences(hardware.used_device, 1, ref (frame.swapchain_fence)));
		swapchain_image = deref(swapchain.images # image_index);
	}

	barrier : VkImageMemoryBarrier mut := VkImageMemoryBarrier
	{
		.sType := 45;
		.pNext := zero;
		.srcAccessMask := 0;
		.dstAccessMask := 0x00001000;//VK_ACCESS_TRANSFER_WRITE_BIT
		.oldLayout := 0;
		.newLayout := 7; // VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL
		.srcQueueFamilyIndex := ~0;
		.dstQueueFamilyIndex := ~0;
		.image := 0;
		.subresourceRange := VkImageSubresourceRange
		{
			.aspectMask := 0x00000001;
			.baseMipLevel := 0;
			.levelCount := 1;
			.baseArrayLayer := 0;
			.layerCount := 1;
		};
	};
	if(will_present)
	{
		(barrier.image) = swapchain_image;
	}

	frame_begin ::= VkCommandBufferBeginInfo
	{
		.sType := 42;
		.pNext := zero;
		.flags := 1;
		.pInheritanceInfo := zero;
	};
	vk_check(vk.begin_command_buffer(frame.cmds, ref frame_begin));

	if(will_present)
	{
		vk.cmd_pipeline_barrier(frame.cmds, 0x00002000, 0x00001000, 0, 0, zero, 0, zero, 1, ref barrier);
	}

	counter : u64 mut;
	cur_entry : graph_entry mut;
	for(counter = 0, counter < (graphptr->timeline_count), counter = counter + 1)
	{
		cur_entry = deref((graphptr->timeline) # counter);
		impl_wait_dependencies(graphptr, counter, short);
		if(cur_entry.is_graph)
		{
			ret = impl_record_subgraph_work((cur_entry.handle)@s64@gpu_graph, long, short);
			if(ret.code != zero)
			{
				return ret;
			}
		}
		else
		{
			ret = impl_record_gpu_work((cur_entry.handle)@s64@gpu_pass, ref(graphptr->pass_record), core.current_frame, long, short);
			if(ret.code != zero)
			{
				return ret;
			}
		}
		impl_notify_dependencies(graphptr, counter);
	}

	// if one of these passes outputted to the system image:
	// 	1. system image must be in color attachment layout. to eventually present later on, we should now transition to transfer_src (to transfer it to swapchain image).
	// 	2. record a command to do the blit (swapchain image should be transfer_dst)
	// 	3. transition the swapchain image to present_src.
	if(writes_to_system_image)
	{
		impl_begin_label(frame.cmds, "Blit System Image -> Swapchain Image");
		defer impl_end_label(frame.cmds);
		blit : VkImageBlit mut := VkImageBlit
		{
			.srcSubresource := VkImageSubresourceLayers
			{
				.aspectMask := 0x00000001;
				.mipLevel := 0;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
			.srcOffsets := zero;
			.dstSubresource := VkImageSubresourceLayers
			{
				.aspectMask := 0x00000001;
				.mipLevel := 0;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
			.dstOffsets := zero;
		};
		deref((blit.srcOffsets) # 0) = VkOffset3D{.x := 0; .y := 0; .z := 0;};
		deref((blit.srcOffsets) # 1) = VkOffset3D{.x := swapchain.width@_; .y := swapchain.height@_; .z := 1;};
		deref((blit.dstOffsets) # 0) = VkOffset3D{.x := 0; .y := 0; .z := 0;};
		deref((blit.dstOffsets) # 1) = VkOffset3D{.x := swapchain.width@_; .y := swapchain.height@_; .z := 1;};

		system_image_transition ::= VkImageMemoryBarrier
		{
			.sType := 45;
			.pNext := zero;
			.srcAccessMask := 0x00000100; // VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT 
			.dstAccessMask := 0x00000800; // VK_ACCESS_TRANSFER_READ_BIT
			.oldLayout := 2; // VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL
			.newLayout := 6; // VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL
			.srcQueueFamilyIndex := ~0;
			.dstQueueFamilyIndex := ~0;
			.image := swapchain.system_image;
			.subresourceRange := VkImageSubresourceRange
			{
				.aspectMask := 0x00000001;
				.baseMipLevel := 0;
				.levelCount := 1;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
		};

		vk.cmd_pipeline_barrier(frame.cmds, 0x00000400, 0x00001000, 0, 0, zero, 0, zero, 1, ref system_image_transition);
		vk.cmd_blit_image(frame.cmds, swapchain.system_image, 6, swapchain_image, 7, 1, ref blit, 0);
	}

	if(will_present)
	{
		barrier.oldLayout = 7;
		barrier.newLayout = 1000001002;
		barrier.srcAccessMask = 0x00001000;
		barrier.dstAccessMask = 0;
		vk.cmd_pipeline_barrier(frame.cmds, 0x00001000, 0x00000001, 0, 0, zero, 0, zero, 1, ref barrier);
	}

	VK_PIPELINE_STAGE_ALL_COMMANDS_BIT ::= 0x00010000;
	vk.cmd_set_event(frame.cmds, graphptr->event, VK_PIPELINE_STAGE_ALL_COMMANDS_BIT);

	vk_check(vk.end_command_buffer(frame.cmds));
	wait_stage : s32 := 0;
	submit ::= VkSubmitInfo
	{
		.sType := 4;
		.pNext := zero;
		.waitSemaphoreCount := 0;
		.pWaitSemaphores := zero;
		.pWaitDstStageMask := ref wait_stage;
		.commandBufferCount := 1;
		.pCommandBuffers := ref (frame.cmds);
		.signalSemaphoreCount := 1;
		.pSignalSemaphores := ref (frame.swapchain_sem);
	};
	vk_check(vk.queue_submit(hardware.graphics_queue, 1, ref submit, frame.swapchain_fence));
	vk_check(vk.wait_for_fences(hardware.used_device, 1, ref (frame.swapchain_fence), 1, -1@u64));
	vk_check(vk.reset_fences(hardware.used_device, 1, ref (frame.swapchain_fence)));

	// present if we need to.
	present_res : s32 mut;
	if(will_present)
	{
		present ::= VkPresentInfoKHR
		{
			.sType := 1000001001;
			.pNext := zero;
			.waitSemaphoreCount := 1;
			.pWaitSemaphores := ref (frame.swapchain_sem);
			.swapchainCount := 1;
			.pSwapchains := ref(swapchain.handle);
			.pImageIndices := ref image_index;
			.pResults := ref present_res;
		};
		vk_check(vk.queue_present_khr(hardware.graphics_queue, ref present));
		vk_check(present_res);
	}

	core.current_frame = ((core.current_frame + 1) % 2);
	return ret;
};

impl_gpu_wait ::= func(g : gpu_graph -> v0)
{
	poll_interval_ms ::= 1;

	graphdata ::= graph.data # (g@s64);
	impl_host_wait_event(graphdata->event, poll_interval_ms);
};

graph_entry ::= struct
{
	handle : u64 mut;
	is_graph : bool mut;
};

graph_data_t ::= struct
{
	name : u8 mut?;
	timeline : graph_entry mut?;
	timeline_count : u64;
	timeline_cap : u64;
	dependencies : u64 mut? mut?;
	dependency_counts : u64 mut?;
	event : u64;

	pass_record : pass_gpu_work_options;
};

// implementation details

impl_graph_will_present ::= func(g : gpu_graph -> bool)
{
	graphptr ::= graph.data # (g@s64);
	count ::= graphptr->timeline_count;

	counter : u64 mut;
	cur_entry : graph_entry mut;
	for(counter = 0, counter < count, counter = counter + 1)
	{
		cur_entry = deref (graphptr->timeline # counter);
		if((cur_entry.handle) == ((gpu_pass.present)@s64@_))
		{
			return true;
		}
	}
	return false;
};

impl_graph_writes_to_system_image ::= func(g : gpu_graph -> bool)
{
	graphptr ::= graph.data # (g@s64);
	count ::= graphptr->timeline_count;

	counter : u64 mut;
	cur_entry : graph_entry mut;
	cur_pass : gpu_pass mut;
	for(counter = 0, counter < count, counter = counter + 1)
	{
		cur_entry = deref (graphptr->timeline # counter);
		if(cur_entry.is_graph)
		{
			if(impl_graph_writes_to_system_image(cur_entry.handle@s64@gpu_graph))
			{
				return true;
			}
		}
		else
		{
			cur_pass = ((cur_entry.handle)@s64@gpu_pass);
			if(impl_pass_writes_to_system_image(cur_pass))
			{
				return true;
			}
		}
	}
	return false;
};

impl_alloc_new_graph ::= func(a : arena mut? -> graph_data_t mut?)
{
	if(graph.cap == 0)
	{
		graph.data = arena_alloc(a, __sizeof(graph_data_t) * 64);
		graph.cap = 64;
	}
	if(graph.count > graph.cap)
	{
		putzstr("ran out of graphs. todo: fix this.");
		__debugbreak();
	}
	id ::= graph.count;
	graph.count = (graph.count + 1);
	return graph.data # id;
};

impl_record_subgraph_work ::= func(g : gpu_graph, long : arena mut?, short : arena mut? -> gpu_err)
{
	return gpu_err{.code := gpu_err_code.unknown; .msg := "subgraphs are nyi";};
};

impl_wait_dependencies ::= func(g : graph_data_t mut?, idx : u64, short : arena mut? -> v0)
{
	my_entry ::= (g->timeline # idx);
	my_handle ::= my_entry->handle;
	if(my_handle == (gpu_pass.present@s64@_))
	{
		return;
	}
	// we're going to iterate over all dependencies
	deps ::= deref(g->dependencies # idx);
	deps_count ::= deref(g->dependency_counts # idx);

	// make an array to store all the events we're gonna wait on
	event_array : u64 mut? := arena_alloc(short, __sizeof(u64) * deps_count);

	// populate event_array
	i : u64 mut;
	skip_count : u64 mut := 0;
	for(i = 0, i < deps_count, i = i + 1)
	{
		depidx ::= deref(deps # (i - skip_count));
		dep_entry ::= (g->timeline # depidx);
		handle ::= dep_entry->handle;
		if(dep_entry->is_graph)
		{
			if(impl_graph_is_real(handle@s64@gpu_graph))
			{
				as_graph ::= deref(graph.data # (handle@s64));
				deref(event_array # i) = (as_graph.event);
			}
			else
			{
				skip_count = skip_count + 1;
			}
		}
		else
		{
			if(impl_pass_is_real(handle@s64@gpu_pass))
			{
				as_pass ::= deref(pass.data # (handle@s64));
				deref(event_array # i) = (as_pass.event);
			}
			else
			{
				skip_count = skip_count + 1;
			}
		}
	}

	VK_PIPELINE_STAGE_ALL_COMMANDS_BIT ::= 0x00010000;
	// do vulkan wait
	frame ::= deref(core.frames # (core.current_frame));
	if(deps_count > 0)
	{
		vk.cmd_wait_events(frame.cmds, (deps_count - skip_count)@_, event_array, VK_PIPELINE_STAGE_ALL_COMMANDS_BIT, VK_PIPELINE_STAGE_ALL_COMMANDS_BIT, zero, zero, zero, zero, zero, zero);
	}
};

impl_notify_dependencies ::= func(g : graph_data_t mut?, idx : u64 -> v0)
{
	event : u64 mut;

	entry ::= (g->timeline # idx);
	handle ::= entry->handle;
	if(entry->is_graph)
	{
		if(!impl_graph_is_real(handle@s64@gpu_graph))
		{
			return;
		}
		as_graph ::= deref(graph.data # (handle@s64));
		event = (as_graph.event);
	}
	else
	{
		if(!impl_pass_is_real(handle@s64@gpu_pass))
		{
			return;
		}
		as_pass ::= deref(pass.data # (handle@s64));
		event = (as_pass.event);
	}
	frame ::= deref(core.frames # (core.current_frame));
	VK_PIPELINE_STAGE_ALL_COMMANDS_BIT ::= 0x00010000;
	vk.cmd_set_event(frame.cmds, event, VK_PIPELINE_STAGE_ALL_COMMANDS_BIT);
};

// true if graph is an actual real graph created via gpu_create_graph, false if e.g present graph or invalid
impl_graph_is_real ::= func(g : gpu_graph -> bool)
{
	return g != (gpu_graph.invalid);
};
