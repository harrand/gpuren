core : core_state mut := zero;

core_state ::= struct
{
	vkinst : u64 mut;
	current_frame : u64 mut;
	target_window : window mut;
	frames : frame_data mut[2] mut;
	scratch : scratch_data mut;
	vallocator : vallocator_t mut;
	initialised : bool;
};

rgba_format ::= 37;
depth_format ::= 126;
frame_overlap ::= 2;
valloc_initial_size : u64 static := (32 * 1024 * 1024); // 32 mib
valloc_t ::= struct
{
	device_mem : u64;
	size : u64;
	cursor : u64;
	mapped_ptr : v0? mut;
};

vallocator_t ::= struct
{
	buffer_gpu : valloc_t;
	image_gpu : valloc_t;
	buffer_cpu : valloc_t;
};

scratch_data ::= struct
{
	cpool : u64;
	cmds : u64;
	fence : u64;
};

frame_data ::= struct
{
	cpool : u64;
	cmds : u64;

	swapchain_fence : u64;
	swapchain_sem : u64;
};

static if(_config == "release")
{
	is_debug_utils_enabled ::= false;
}
else
{
	is_debug_utils_enabled ::= true;
}

impl_gpu_init ::= func(info : gpu_appinfo -> gpu_err)
{
	if(core.initialised)
	{
		return gpu_err
		{
			.code := gpu_err_code.malformed;
			.msg := "gpu_init has already been called!";
		};
	}
	GPU_VERSION ::= VK_MAKE_API_VERSION(0, 0, 1, 0);
	vulkan_init();
	core.target_window = window.invalid;

	appinfo ::= VkApplicationInfo
	{
		.sType := 0;
		.pNext := zero;
		.pApplicationName := info.name;
		.applicationVersion := VK_MAKE_API_VERSION(0, info.ver_maj, info.ver_min, 0);
		.pEngineName := "gpu";
		.engineVersion := GPU_VERSION;
		.apiVersion := VK_MAKE_API_VERSION(0, 1, 3, 0);
	};

	extensions : u8? mut[3];
	deref(extensions # 0) = "VK_KHR_surface";
	static if(_win32)
	{
		deref(extensions # 1) = "VK_KHR_win32_surface";
	}
	static if(_linux)
	{
		deref(extensions # 1) = "VK_KHR_xlib_surface";
	}
	deref(extensions # 2) = "VK_EXT_debug_utils";
	extension_count : u32 mut := __sizeof(extensions) / __sizeof(u8?);

	layers : u8? mut[1];
	deref(layers # 0) = "VK_LAYER_KHRONOS_validation";

	static if(_config == "release")
	{
		layer_count ::= 0;
		extension_count = (extension_count - 1);
	}
	else
	{
		layer_count ::= 1;
		putzstr("validation layers enabled");
		putchar(10);
	}

	create ::= VkInstanceCreateInfo
	{
		.sType := 1;
		.pNext := zero;
		.flags := 0;
		.pApplicationInfo := ref appinfo;
		.enabledLayerCount := layer_count@_;
		.ppEnabledLayerNames := (layers # 0);
		.enabledExtensionCount := extension_count;
		.ppEnabledExtensionNames := (extensions # 0);
	};

	create_result ::= vk.create_instance(ref create, zero, ref(core.vkinst));
	if(create_result != zero)
	{
		if(create_result == (VkResult.VK_ERROR_EXTENSION_NOT_PRESENT@s64@_))
		{
			return gpu_err
			{
				.code := gpu_err_code.reqfailed;
				.msg := "vulkan instance creation failed - a required extension is not supported";
			};
		}
		if(create_result == (VkResult.VK_ERROR_LAYER_NOT_PRESENT@s64@_))
		{
			return gpu_err
			{
				.code := gpu_err_code.reqfailed;
				.msg := "vulkan instance creation failed - a required layer is not supported";
			};
		}
		if(create_result == (VkResult.VK_ERROR_INCOMPATIBLE_DRIVER@s64@_))
		{
			return gpu_err
			{
				.code := gpu_err_code.reqfailed;
				.msg := "vulkan instance creation failed - incompatible driver";
			};
		}
		if(create_result == (VkResult.VK_ERROR_OUT_OF_HOST_MEMORY@s64@_))
		{
			return gpu_err
			{
				.code := gpu_err_code.cpuoom;
				.msg := "vulkan instance creation failed - out of host memory";
			};
		}
		if(create_result == (VkResult.VK_ERROR_OUT_OF_DEVICE_MEMORY@s64@_))
		{
			return gpu_err
			{
				.code := gpu_err_code.gpuoom;
				.msg := "vulkan instance creation failed - out of device memory";
			};
		}
		// VK_ERROR_INTIALIZATION_FAILED
		// VK_ERROR_UNKNOWN
		// VK_ERROR_VALIDATION_FAILED
		return gpu_err
		{
			.code := gpu_err_code.unknown;
			.msg := "vulkan instance creation failed - internal error";
		};
	}
	vulkan_load_functions(core.vkinst, is_debug_utils_enabled);
	core.initialised = true;
	return zero;
};

impl_gpu_term ::= func( -> gpu_err)
{
	if(!(core.initialised))
	{
		return gpu_err
		{
			.code := gpu_err_code.malformed;
			.msg := "call to gpu_term but we are not gpu_init'd!";
		};
	}
	vk.device_wait_idle(hardware.used_device);
	if(hardware.pipeline_layout != 0)
	{
		vk.destroy_pipeline_layout(hardware.used_device, hardware.pipeline_layout, zero);
		hardware.pipeline_layout = 0;
	}
	if(core.scratch.cpool != 0)
	{
		vk.destroy_command_pool(hardware.used_device, core.scratch.cpool, zero);
		core.scratch.cpool = 0;
	}
	if(core.scratch.fence != 0)
	{
		vk.destroy_fence(hardware.used_device, core.scratch.fence, zero);
	}
	counter : u64 mut;
	frame_ptr : frame_data mut? mut;
	for(counter = 0, counter < frame_overlap, counter = counter + 1)
	{
		frame_ptr = (core.frames # counter);
		if(frame_ptr->cpool != 0)
		{
			vk.destroy_command_pool(hardware.used_device, frame_ptr->cpool, zero);
			(frame_ptr->cpool) = 0;
		}
		if(frame_ptr->swapchain_fence != 0)
		{
			vk.destroy_fence(hardware.used_device, frame_ptr->swapchain_fence, zero);
			(frame_ptr->swapchain_fence) = 0;
		}
		if(frame_ptr->swapchain_sem != 0)
		{
			vk.destroy_semaphore(hardware.used_device, frame_ptr->swapchain_sem, zero);
			(frame_ptr->swapchain_sem) = 0;
		}

		// dont forget descriptor layouts.
		vk.destroy_descriptor_set_layout(hardware.used_device, deref(descriptor.set_layouts # counter), zero);
		deref(descriptor.set_layouts # counter) = 0;
	}
	for(counter = 0, counter < (swapchain.image_count@_), counter = counter + 1)
	{
		vk.destroy_image_view(hardware.used_device, deref(swapchain.views # counter), zero);
	}
	if(swapchain.handle != 0)
	{
		vk.destroy_swapchain_khr(hardware.used_device, swapchain.handle, zero);
	}

	if(swapchain.system_image_view != 0)
	{
		vk.destroy_image_view(hardware.used_device, swapchain.system_image_view, zero);
		swapchain.system_image_view = 0;
	}
	if(swapchain.system_image != 0)
	{
		vk.destroy_image(hardware.used_device, swapchain.system_image, zero);
		swapchain.system_image = 0;
	}

	if(swapchain.system_depth_image_view != 0)
	{
		vk.destroy_image_view(hardware.used_device, swapchain.system_depth_image_view, zero);
		swapchain.system_depth_image_view = 0;
	}
	if(swapchain.system_depth_image != 0)
	{
		vk.destroy_image(hardware.used_device, swapchain.system_depth_image, zero);
		swapchain.system_depth_image = 0;
	}

	// destroy all graphs
	graphptr : graph_data_t mut? mut;
	for(counter = 0, counter < (graph.count), counter = counter + 1)
	{
		graphptr = graph.data # counter;
		vk.destroy_event(hardware.used_device, graphptr->event, zero);
	}

	// destroy all passes
	passptr : pass_data_t mut? mut;
	for(counter = 0, counter < (pass.count), counter = counter + 1)
	{
		passptr = pass.data # counter;	
		vk.destroy_buffer(hardware.used_device, passptr->metabuf, zero);
		vk.destroy_pipeline(hardware.used_device, passptr->pipeline, zero);
		vk.destroy_event(hardware.used_device, passptr->event, zero);
	}
	pass.count = 0;

	// destroy all resources.
	resptr : resource_data_t mut? mut;
	for(counter = 0, counter < (resource.count), counter = counter + 1)
	{
		resptr = (resource.data # counter);	
		if(resptr->is_buffer)
		{
			vk.destroy_buffer(hardware.used_device, resptr->vk_handle, zero);
			(resptr->vk_handle) = 0;
		}
		else
		{
			vk.destroy_sampler(hardware.used_device, resptr->sampler, zero);
			vk.destroy_image_view(hardware.used_device, resptr->image_view, zero);
			vk.destroy_image(hardware.used_device, resptr->vk_handle, zero);
			(resptr->vk_handle) = 0;
		}
	}
	resource.count = 0;

	for(counter = 0, counter < (descriptor.pools_count), counter = counter + 1)
	{
		vk.destroy_descriptor_pool(hardware.used_device, deref(descriptor.pools_data # counter), zero);
	}
	descriptor.pools_count = 0;

	// destroy all shaders.
	shad : shader_data_t mut? mut;
	for(counter = 0, counter < (shader.count), counter = counter + 1)
	{
		shad = (shader.data # counter);
		if(shad->vertex_module != 0)
		{
			vk.destroy_shader_module(hardware.used_device, shad->vertex_module, zero);
			shad->vertex_module = 0;
		}
		if(shad->fragment_module != 0)
		{
			vk.destroy_shader_module(hardware.used_device, shad->fragment_module, zero);
			shad->fragment_module = 0;
		}
		if(shad->compute_module != 0)
		{
			vk.destroy_shader_module(hardware.used_device, shad->compute_module, zero);
			shad->compute_module = 0;
		}
	}
	shader.count = 0;

	impl_vallocator_free_all();

	vk.destroy_device(hardware.used_device, zero);
	if(swapchain.surface != 0)
	{
		vk.destroy_surface_khr(core.vkinst, swapchain.surface, zero);
	}
	vk.destroy_instance(core.vkinst, zero);
	vulkan_term();
	core.initialised = false;
	return zero;
};

impl_vallocator_initial_setup ::= func( -> gpu_err)
{
	bda_flags ::= VkMemoryAllocateFlagsInfo
	{
		.sType := 1000060000;
		.pNext := zero;
		.flags := 0x00000002;
		.deviceMask := 0;
	};
	gpucreate ::= VkMemoryAllocateInfo
	{
		.sType := 5;
		.pNext := (ref bda_flags)@_;
		.allocationSize := valloc_initial_size;
		.memoryTypeIndex := hardware.used_mti_gpu;
	};
	cpucreate ::= VkMemoryAllocateInfo
	{
		.sType := 5;
		.pNext := (ref bda_flags)@_;
		.allocationSize := valloc_initial_size;
		.memoryTypeIndex := hardware.used_mti_cpu;
	};
	ret : s32 mut := zero;

	// create initial allocations for each case (buffer gpu, image gpu, buffer cpu).
	bgpu ::= ref(core.vallocator.buffer_gpu);
	ret = vk.allocate_memory(hardware.used_device, ref gpucreate, zero, ref (bgpu->device_mem));
	if(ret != zero)
	{
		if(ret == (VkResult.VK_ERROR_OUT_OF_DEVICE_MEMORY@s64@_))
		{
			return gpu_err
			{
				.code := gpu_err_code.gpuoom;
				.msg := "failed to create vulkan buffer-gpu allocation - out of device memory";
			};
		}
		else
		{
			return gpu_err
			{
				.code := gpu_err_code.unknown;
				.msg := "failed to create vulkan buffer-gpu allocation - internal error";
			};
		}
	}
	bgpu->size = valloc_initial_size;
	bgpu->cursor = 0;

	impl_label_device_memory(hardware.used_device, bgpu->device_mem, "Buffer GPU Memory");
	igpu ::= ref(core.vallocator.image_gpu);
	ret = vk.allocate_memory(hardware.used_device, ref gpucreate, zero, ref (igpu->device_mem));
	if(ret != zero)
	{
		if(ret == (VkResult.VK_ERROR_OUT_OF_DEVICE_MEMORY@s64@_))
		{
			return gpu_err
			{
				.code := gpu_err_code.gpuoom;
				.msg := "failed to create vulkan image-gpu allocation - out of device memory";
			};
		}
		return gpu_err
		{
			.code := gpu_err_code.unknown;
			.msg := "failed to create vulkan image-gpu allocation - internal error";
		};
	}
	igpu->size = valloc_initial_size;
	igpu->cursor = 0;
	impl_label_device_memory(hardware.used_device, igpu->device_mem, "Image GPU Memory");

	bcpu ::= ref(core.vallocator.buffer_cpu);
	ret = vk.allocate_memory(hardware.used_device, ref cpucreate, zero, ref (bcpu->device_mem));
	if(ret != zero)
	{
		if(ret == (VkResult.VK_ERROR_OUT_OF_HOST_MEMORY@s64@_))
		{
			return gpu_err
			{
				.code := gpu_err_code.cpuoom;
				.msg := "failed to create vulkan buffer-cpu allocation - out of host memory";
			};
		}
		return gpu_err
		{
			.code := gpu_err_code.unknown;
			.msg := "failed to create vulkan buffer-cpu allocation - internal error";
		};
	}
	ret = vk.map_memory(hardware.used_device, bcpu->device_mem, 0, valloc_initial_size, 0, ref (bcpu->mapped_ptr));
	if(ret != zero)
	{
		if(ret == (VkResult.VK_ERROR_OUT_OF_HOST_MEMORY@s64@_))
		{
			return gpu_err
			{
				.code := gpu_err_code.cpuoom;
				.msg := "failed to persistently-map vulkan buffer-cpu allocation - out of host memory";
			};
		}
		return gpu_err
		{
			.code := gpu_err_code.unknown;
			.msg := "failed to persistently-map vulkan buffer-cpu allocation - internal error";
		};
	}
	
	bcpu->size = valloc_initial_size;
	bcpu->cursor = 0;
	impl_label_device_memory(hardware.used_device, bcpu->device_mem, "Buffer CPU Memory");
	return zero;
};


impl_vallocator_free_all ::= func( -> v0)
{
	bgpu ::= core.vallocator.buffer_gpu;
	vk.free_memory(hardware.used_device, bgpu.device_mem, zero);
	igpu ::= core.vallocator.image_gpu;
	vk.free_memory(hardware.used_device, igpu.device_mem, zero);

	bcpu ::= core.vallocator.buffer_cpu;
	vk.free_memory(hardware.used_device, bcpu.device_mem, zero);
};


impl_bind_image_mem ::= func(image : u64 -> gpu_err)
{
	valloc : valloc_t mut? mut := ref (core.vallocator.image_gpu);

	memreqs : VkMemoryRequirements mut;
	vk.get_image_memory_requirements(hardware.used_device, image, ref memreqs);

	imgsize ::= memreqs.size;
	if((valloc->cursor + imgsize) > (valloc->size))
	{
		return gpu_err
		{
			.code := gpu_err_code.gpuoom;
			.msg := "vulkan image allocation was too large so we have ran out of space";
		};
	}

	padding ::= ((memreqs.align) - ((valloc->cursor) % (memreqs.align)) % (memreqs.align));
	(valloc->cursor) = (valloc->cursor) + (padding);

	ret ::= vk.bind_image_memory(hardware.used_device, image, valloc->device_mem, valloc->cursor);
	if(ret == -1) // out of host memory 
	{
		putzstr("oom when binding image memory. todo: multiple allocations");
		__debugbreak();
	}
	if(ret == -2) // out of device memory 
	{
		putzstr("voom when binding image memory. todo: multiple allocations");
		__debugbreak();
	}
	// todo: align safely.
	valloc->cursor = (valloc->cursor) + imgsize;
	return zero;
};

impl_bind_buffer_mem ::= func(buffer : u64, bufsize : u64, gpu_memory : bool, err : gpu_err mut? -> u64)
{
	valloc : valloc_t mut? mut := ref (core.vallocator.buffer_cpu);
	if(gpu_memory)
	{
		valloc = ref(core.vallocator.buffer_gpu);
	}
	if((valloc->cursor + bufsize) > (valloc->size))
	{
		errcode : gpu_err_code mut := gpu_err_code.cpuoom;
		if(gpu_memory)
		{
			errcode = gpu_err_code.gpuoom;
		}
		deref(err) = gpu_err
		{
			.code := errcode;
			.msg := "vulkan buffer allocation was too large so we have ran out of space";
		};
		return -1;
	}

	memreqs : VkMemoryRequirements mut;
	vk.get_buffer_memory_requirements(hardware.used_device, buffer, ref memreqs);
	padding ::= ((memreqs.align) - ((valloc->cursor) % (memreqs.align)) % (memreqs.align));
	(valloc->cursor) = (valloc->cursor) + (padding);
	offset_from_zero ::= valloc->cursor;

	ret ::= vk.bind_buffer_memory(hardware.used_device, buffer, valloc->device_mem, valloc->cursor);
	if(ret == -1) // out of host memory 
	{
		putzstr("oom when binding buffer memory. todo: multiple allocations");
		__debugbreak();
	}
	if(ret == -2) // out of device memory 
	{
		putzstr("voom when binding buffer memory. todo: multiple allocations");
		__debugbreak();
	}
	(valloc->cursor) = (valloc->cursor) + bufsize;
	deref(err) = zero;
	return offset_from_zero;
};

impl_create_graphics_pipeline ::= func(s : gpu_shader, graphics : gpu_graphics_state, a : arena mut? -> u64)
{
	shadptr ::= (shader.data # (s@s64));
	vertex_shader_module ::= shadptr->vertex_module;
	if(vertex_shader_module == 0)
	{
		putzstr("invalid vertex shader");
		__debugbreak();
	}
	fragment_shader_module ::= shadptr->fragment_module;
	if(fragment_shader_module == 0)
	{
		putzstr("invalid fragment shader");
		__debugbreak();
	}

	shader_creates : VkPipelineShaderStageCreateInfo mut[2];
	deref(shader_creates # 0) = VkPipelineShaderStageCreateInfo
	{
		.sType := 18;
		.pNext := zero;
		.flags := 0;
		.stage := 0x00000001;
		.module := vertex_shader_module;
		.pName := "main";
		.pSpecializationInfo := zero;

	};
	deref(shader_creates # 1) = VkPipelineShaderStageCreateInfo
	{
		.sType := 18;
		.pNext := zero;
		.flags := 0;
		.stage := 0x00000010;
		.module := fragment_shader_module;
		.pName := "main";
		.pSpecializationInfo := zero;
	};

	vtx ::= VkPipelineVertexInputStateCreateInfo
	{
		.sType := 19;
		.pNext := zero;
		.flags := 0;
		.vertexBindingDescriptionCount := 0;
		.pVertexBindingDescriptions := zero;
		.vertexAttributeDescriptionCount := 0;
		.pVertexAttributeDescriptions := zero;
	};

	iasm ::= VkPipelineInputAssemblyStateCreateInfo
	{
		.sType := 20;
		.pNext := zero;
		.flags := 0;
		.topology := 3;
		.primitiveRestartEnable := 0;
	};

	tess ::= VkPipelineTessellationStateCreateInfo
	{
		.sType := 21;
		.pNext := zero;
		.flags := 0;
		.patchControlPoints := 3;
	};

	vwprt ::= VkPipelineViewportStateCreateInfo
	{
		.sType := 22;
		.pNext := zero;
		.flags := 0;
		.viewportCount := 1;
		//remember: this is dynamic state so we pass zeros even though there will be a viewport and scissor.
		.pViewports := zero;
		.scissorCount := 1;
		.pScissors := zero;
	};

	cull_bits : s32 mut;
	if(graphics.culling == (gpu_cull.both))
	{
		cull_bits = 3;
	}
	if(graphics.culling == (gpu_cull.front))
	{
		cull_bits = 1;
	}
	if(graphics.culling == (gpu_cull.back))
	{
		cull_bits = 2;
	}
	if(graphics.culling == (gpu_cull.none))
	{
		cull_bits = 0;
	}

	raster ::= VkPipelineRasterizationStateCreateInfo
	{
		.sType := 23;
		.pNext := zero;
		.flags := 0;
		.depthClampEnable := 0;
		.rasterizerDiscardEnable := 0;
		.polygonMode := 0;
		.cullMode := cull_bits;
		.frontFace := 0;
		.depthBiasEnable := 0;
		.depthBiasConstantFactor := 0.0;
		.depthBiasClamp := 0.0;
		.depthBiasSlopeFactor := 0.0;
		.lineWidth := 1.0;
	};

	multi ::= VkPipelineMultisampleStateCreateInfo
	{
		.sType := 24;
		.pNext := zero;
		.flags := 0;
		.rasterizationSamples := 1;
		.sampleShadingEnable := 0;
		.minSampleShading := 1.0;
		.pSampleMask := zero;
		.alphaToCoverageEnable := 0;
		.alphaToOneEnable := 0;
	};

	depth_stencil ::= VkPipelineDepthStencilStateCreateInfo
	{
		.sType := 25;
		.pNext := zero;
		.flags := 0;
		.depthTestEnable := (graphics.depth_target != (gpu_resource.invalid))@_;
		.depthWriteEnable := (graphics.depth_target != (gpu_resource.invalid))@_;
		.depthCompareOp := 1;
		.depthBoundsTestEnable := 0;
		.stencilTestEnable := 0;
		.front := zero;
		.back := zero;
		.minDepthBounds := 0.0;
		.maxDepthBounds := 1.0;
	};

	counter : u64 mut;
	blend_states : VkPipelineColorBlendAttachmentState mut? := arena_alloc(a, __sizeof(VkPipelineColorBlendAttachmentState) * (graphics.colour_targets_count));
	for(counter = 0, counter < (graphics.colour_targets_count), counter = counter + 1)
	{
		deref(blend_states # counter) = VkPipelineColorBlendAttachmentState
		{
			.blendEnable := 0;
			.srcColorBlendFactor := 1;
			.dstColorBlendFactor := 0;
			.colorBlendOp := 0;
			.srcAlphaBlendFactor := 1;
			.dstAlphaBlendFactor := 0;
			.alphaBlendOp := 0;
			.colorWriteMask := (0x00000001 | 0x00000002 | 0x00000004 | 0x00000008);
		};
	}

	blend ::= VkPipelineColorBlendStateCreateInfo
	{
		.sType := 26;
		.pNext := zero;
		.flags := 0;
		.logicOpEnable := 0;
		.logicOp := 0;
		.attachmentCount := (graphics.colour_targets_count)@_;
		.pAttachments := blend_states # 0;
	};
	deref((blend.blendConstants) # 0) = 0.0;
	deref((blend.blendConstants) # 1) = 0.0;
	deref((blend.blendConstants) # 2) = 0.0;
	deref((blend.blendConstants) # 3) = 0.0;

	color_formats : s32 mut? := arena_alloc(a, __sizeof(s32) * (graphics.colour_targets_count));
	for(counter = 0, counter < (graphics.colour_targets_count), counter = counter + 1)
	{
		deref(color_formats # counter) = rgba_format;
	}
	counter = 0;

	rendering ::= VkPipelineRenderingCreateInfo
	{
		.sType := 1000044002;
		.pNext := zero;
		.viewMask := 0;
		.colorAttachmentCount := (graphics.colour_targets_count)@_;
		.pColorAttachmentFormats := color_formats;
		.depthAttachmentFormat := 126;
		.stencilAttachmentFormat := 0;
	};

	dynamic_states : s32 mut[2];
	deref(dynamic_states # 0) = 0; // VK_DYNAMIC_STATE_VIEWPORT
	deref(dynamic_states # 1) = 1; // VK_DYNAMIC_STATE_SCISSOR

	dyn ::= VkPipelineDynamicStateCreateInfo
	{
		.sType := 27;
		.pNext := zero;
		.flags := 0;
		.dynamicStateCount := (__sizeof(dynamic_states) / __sizeof(deref(dynamic_states # 0)));
		.pDynamicStates := dynamic_states # 0;
	};

	create ::= VkGraphicsPipelineCreateInfo
	{
		.sType := 28;
		.pNext := (ref rendering)@_;
		.flags := 0;
		.stageCount := 2;
		.pStages := shader_creates # 0;
		.pVertexInputState := ref vtx;
		.pInputAssemblyState := ref iasm;
		.pTessellationState := ref tess;
		.pViewportState := ref vwprt;
		.pRasterizationState := ref raster;
		.pMultisampleState := ref multi;
		.pDepthStencilState := ref depth_stencil;
		.pColorBlendState := ref blend;
		.pDynamicState := ref dyn;
		.layout := hardware.pipeline_layout;
		.renderPass := 0;
		.subpass := 0;
		.basePipelineHandle := 0;
		.basePipelineIndex := -1;
	};

	ret : u64 mut;
	vk_check(vk.create_graphics_pipelines(hardware.used_device, 0, 1, ref create, zero, ref ret));
	return ret;
};

impl_create_compute_pipeline ::= func(s : gpu_shader, compute : gpu_compute_state -> u64)
{
	shadptr ::= (shader.data # (s@s64));
	compute_shader_module ::= shadptr->compute_module;
	if(compute_shader_module == 0)
	{
		putzstr("invalid compute shader");
		__debugbreak();
	}
	create ::= VkComputePipelineCreateInfo
	{
		.sType := 29;
		.pNext := zero;
		.flags := 0;
		.stage := VkPipelineShaderStageCreateInfo
		{
			.sType := 18;
			.pNext := zero;
			.flags := 0;
			.stage := 0x00000020; 
			.module := compute_shader_module;
			.pName := "main";
			.pSpecializationInfo := zero;
		};
		.layout := hardware.pipeline_layout;
		.basePipelineHandle := 0;
		.basePipelineIndex := -1;
	};
	ret : u64 mut;

	vk_check(vk.create_compute_pipelines(hardware.used_device, 0, 1, ref create, zero, ref ret));
	return ret;
};

impl_begin_scratch_commands ::= func( -> v0)
{
	create ::= VkCommandBufferBeginInfo
	{
		.sType := 42;
		.pNext := zero;
		.flags := 0x00000001; //VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT
		.pInheritanceInfo := zero;
	};

	vk_check(vk.begin_command_buffer(core.scratch.cmds, ref create));
};

impl_end_and_execute_scratch_commands ::= func( -> v0)
{
	vk_check(vk.end_command_buffer(core.scratch.cmds));
	submit ::= VkSubmitInfo
	{
		.sType := 4;
		.pNext := zero;
		.waitSemaphoreCount := 0;
		.pWaitSemaphores := zero;
		.pWaitDstStageMask := zero;
		.commandBufferCount := 1;
		.pCommandBuffers := ref (core.scratch.cmds);
		.signalSemaphoreCount := 0;
		.pSignalSemaphores := zero;
	};
	vk_check(vk.queue_submit(hardware.graphics_queue, 1, ref submit, core.scratch.fence));
	vk_check(vk.wait_for_fences(hardware.used_device, 1, ref (core.scratch.fence), 1, -1@u64));
	vk_check(vk.reset_fences(hardware.used_device, 1, ref (core.scratch.fence)));
};

static if(_win32)
{
	Sleep ::= func(dwMilliseconds : u32 -> v0) := extern;
}

impl_host_wait_event ::= func(event : u64, poll_interval_ms : u32 -> v0)
{
	while(true)
	{
		result ::= vk.get_event_status(hardware.used_device, event);
		if(result == (VkResult.VK_EVENT_SET@s64@_))
		{
			return;
		}
		// sleepy time (or spin...)
		static if(_win32)
		{
			Sleep(poll_interval_ms);
		}
		else
		{
			__error("todo: fixme on linux");
		}
	}
};
