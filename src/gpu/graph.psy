graph : graph_state mut := zero;
graph_state ::= struct
{
	data : graph_data_t mut?;
	count : u64 := 0;
	cap : u64 := 0;
};

impl_graph_info_deep_copy ::= func(info : gpu_graph_info, a : arena mut? -> gpu_graph_info)
{
	namelen ::= zstrlen(info.name);
	ret : gpu_graph_info mut := info;
	ret.name = arena_alloc(a, namelen + 1);
	memcopy(ret.name, info.name, namelen);
	return ret;
};

impl_gpu_create_graph ::= func(info : gpu_graph_info, a : arena mut? -> gpu_graph)
{
	ret ::= graph.count;
	graphptr ::= impl_alloc_new_graph(a);
	graphptr->info = impl_graph_info_deep_copy(info, a);
	graphptr->pass_record = zero;

	initial_timeline_capacity ::= 8;
	graphptr->timeline = arena_alloc(a, __sizeof(graph_entry) * initial_timeline_capacity);
	graphptr->timeline_count = 0;
	graphptr->timeline_cap = initial_timeline_capacity;
	graphptr->dependencies = arena_alloc(a, __sizeof(u64 mut?) * initial_timeline_capacity);
	graphptr->dependency_counts = arena_alloc(a, __sizeof(u64) * initial_timeline_capacity);

	graphptr->event = impl_create_empty_event();
	return ret@gpu_graph;
};

impl_graph_add_empty_element ::= func(g : gpu_graph, dependencies : u64?, dependencies_count : u64, a : arena mut? -> u64)
{
	graphptr ::= graph.data # (g@s64);
	// create a new entry in the graph
	// make sure there's space for it
	while((graphptr->timeline_count) >= (graphptr->timeline_cap))
	{
		olddata ::= graphptr->timeline;
		oldcap ::= graphptr->timeline_cap;
		graphptr->timeline_cap = (oldcap * 2);
		graphptr->timeline = arena_alloc(a, __sizeof(graph_entry) * (graphptr->timeline_cap));
		memcopy(graphptr->timeline, olddata, __sizeof(graph_entry) * oldcap);

		olddepdata ::= graphptr->dependencies;
		olddepcountdata ::= graphptr->dependency_counts;
		// remember we do indeed use oldcap here, not dependencies_count
		graphptr->dependencies = arena_alloc(a, __sizeof(u64 mut?) * (graphptr->timeline_cap));
		memcopy(graphptr->dependencies, olddepdata, __sizeof(u64 mut?) * oldcap);
		graphptr->dependency_counts = arena_alloc(a, __sizeof(u64) * (graphptr->timeline_cap));
		memcopy(graphptr->dependency_counts, olddepcountdata, __sizeof(u64) * oldcap);
	}
	// this is the index of the new entry (pass/subgraph/operation i dont care here)
	idx ::= graphptr->timeline_count;
	(graphptr->timeline_count) = (graphptr->timeline_count) + 1;

	// i know how many dependencies this new entry has
	// and i know which other entries we depend on
	// so i have enough information to create the execution dependency
	// so i can create the dependency array and populate the indices
	// but i will leave the resource dependencies zerod out.
	deps_array : graph_dependency_data_t mut? := arena_alloc(a, __sizeof(graph_dependency_data_t) * dependencies_count);
	i : u64 mut;
	for(i = 0, i < dependencies_count, i = i + 1)
	{
		deref(deps_array # i) = graph_dependency_data_t
		{
			.idx := deref(dependencies # i);
			// i dont know these right now i will sort that later.
			.resource_dependencies := zero;
			.resource_dependencies_count := zero;
		};
	}

	deref(graphptr->dependencies # idx) = deps_array;
	deref(graphptr->dependency_counts # idx) = dependencies_count;

	return idx;
};

impl_gpu_graph_add_subgraph ::= func(g : gpu_graph, subgraph : gpu_graph, dependencies : u64?, dependencies_count : u64, a : arena mut? -> u64)
{
	if(subgraph == (gpu_graph.invalid))
	{
		putzstr("fatal error: passed gpu_graph.invalid to gpu_graph_add_subgraph");
	}
	graphptr ::= graph.data # (g@s64);
	idx ::= impl_graph_add_empty_element(g, dependencies, dependencies_count, a);
	entry ::= graphptr->timeline # idx;
	entry->handle = (g@s64@_);
	entry->type = (graph_entry_type.subgraph);
	impl_graph_entry_populate_resource_dependencies(g, idx);
	return idx;
};

impl_gpu_graph_add_pass ::= func(g : gpu_graph, p : gpu_pass, dependencies : u64?, dependencies_count : u64, a : arena mut? -> u64)
{
	if(p == (gpu_pass.invalid))
	{
		putzstr("fatal error: passed gpu_pass.invalid to gpu_graph_add_pass");
	}
	graphptr ::= graph.data # (g@s64);
	idx ::= impl_graph_add_empty_element(g, dependencies, dependencies_count, a);
	entry ::= graphptr->timeline # idx;
	entry->handle = (p@s64@_);
	entry->type = (graph_entry_type.pass);
	impl_graph_entry_populate_resource_dependencies(g, idx);
	return idx;
};

impl_gpu_graph_add_operation ::= func(g : gpu_graph, o : gpu_operation, dependencies : u64?, dependencies_count : u64, a : arena mut? -> u64)
{
	if(o == (gpu_operation.invalid))
	{
		putzstr("fatal error: passed gpu_operation.invalid to gpu_graph_add_operation");
	}
	graphptr ::= graph.data # (o@s64);
	idx ::= impl_graph_add_empty_element(g, dependencies, dependencies_count, a);
	entry ::= graphptr->timeline # idx;
	entry->handle = (o@s64@_);
	entry->type = (graph_entry_type.operation);
	impl_graph_entry_populate_resource_dependencies(g, idx);
	return idx;
};

impl_gpu_execute ::= func(g : gpu_graph, long : arena mut?, short : arena mut? -> gpu_err)
{
	ret : gpu_err mut := zero;
	graphptr ::= graph.data # (g@s64);
	impl_pass_gpu_clear_views(ref(graphptr->pass_record));
	vk.reset_event(hardware.used_device, graphptr->event);
	writes_to_system_image ::= impl_graph_writes_to_system_image(g);
	will_present ::= impl_graph_will_present(g);

	frame ::= deref(core.frames # (core.current_frame));

	image_index : u32 mut := -1@u32;
	swapchain_image : u64 mut := 0;
	if(will_present)
	{
		// make sure swapchain is available.
		swapchain_err ::= impl_require_swapchain(long);
		if(swapchain_err.code != zero)
		{
			return swapchain_err;
		}
		if(swapchain.width == 0)
		{
			return zero;
		}
		if(swapchain.height == 0)
		{
			return zero;
		}

		vk_check(vk.acquire_next_image_khr(hardware.used_device, swapchain.handle, 9999999999, 0, frame.swapchain_fence, ref image_index));
		vk_check(vk.wait_for_fences(hardware.used_device, 1, ref (frame.swapchain_fence), 1, ~0));
		vk_check(vk.reset_fences(hardware.used_device, 1, ref (frame.swapchain_fence)));
		swapchain_image = deref(swapchain.images # image_index);
	}

	barrier : VkImageMemoryBarrier mut := VkImageMemoryBarrier
	{
		.sType := 45;
		.pNext := zero;
		.srcAccessMask := 0;
		.dstAccessMask := 0x00001000;//VK_ACCESS_TRANSFER_WRITE_BIT
		.oldLayout := 0;
		.newLayout := 7; // VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL
		.srcQueueFamilyIndex := ~0;
		.dstQueueFamilyIndex := ~0;
		.image := 0;
		.subresourceRange := VkImageSubresourceRange
		{
			.aspectMask := 0x00000001;
			.baseMipLevel := 0;
			.levelCount := 1;
			.baseArrayLayer := 0;
			.layerCount := 1;
		};
	};
	if(will_present)
	{
		(barrier.image) = swapchain_image;
	}

	frame_begin ::= VkCommandBufferBeginInfo
	{
		.sType := 42;
		.pNext := zero;
		.flags := 1;
		.pInheritanceInfo := zero;
	};
	vk_check(vk.begin_command_buffer(frame.cmds, ref frame_begin));

	if(will_present)
	{
		vk.cmd_pipeline_barrier(frame.cmds, 0x00002000, 0x00001000, 0, 0, zero, 0, zero, 1, ref barrier);
	}

	counter : u64 mut;
	cur_entry : graph_entry mut;
	for(counter = 0, counter < (graphptr->timeline_count), counter = counter + 1)
	{
		cur_entry = deref((graphptr->timeline) # counter);
		impl_wait_dependencies(graphptr, counter, short);
		ty ::= cur_entry.type;
		if(ty == (graph_entry_type.subgraph))
		{
			ret = impl_record_subgraph_work((cur_entry.handle)@s64@gpu_graph, long, short);
		}
		else
		{
			if(ty == (graph_entry_type.pass))
			{
				ret = impl_record_gpu_work((cur_entry.handle)@s64@gpu_pass, ref(graphptr->pass_record), core.current_frame, long, short);
			}
			else
			{
				// must be an operation then
				ret = impl_record_operation_work((cur_entry.handle)@s64@gpu_operation, core.current_frame);
			}
		}
		if(ret.code != zero)
		{
			return ret;
		}
		impl_notify_dependencies(graphptr, counter);
	}

	// if one of these passes outputted to the system image (which we want to present):
	// 	1. system image must be in color attachment layout. to eventually present later on, we should now transition to transfer_src (to transfer it to swapchain image).
	// 	2. record a command to do the blit (swapchain image should be transfer_dst)
	// 	3. transition the swapchain image to present_src.
	if(will_present)
	{
		impl_begin_label(frame.cmds, "Blit System Image -> Swapchain Image");
		defer impl_end_label(frame.cmds);
		blit : VkImageBlit mut := VkImageBlit
		{
			.srcSubresource := VkImageSubresourceLayers
			{
				.aspectMask := 0x00000001;
				.mipLevel := 0;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
			.srcOffsets := zero;
			.dstSubresource := VkImageSubresourceLayers
			{
				.aspectMask := 0x00000001;
				.mipLevel := 0;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
			.dstOffsets := zero;
		};
		deref((blit.srcOffsets) # 0) = VkOffset3D{.x := 0; .y := 0; .z := 0;};
		deref((blit.srcOffsets) # 1) = VkOffset3D{.x := swapchain.width@_; .y := swapchain.height@_; .z := 1;};
		deref((blit.dstOffsets) # 0) = VkOffset3D{.x := 0; .y := 0; .z := 0;};
		deref((blit.dstOffsets) # 1) = VkOffset3D{.x := swapchain.width@_; .y := swapchain.height@_; .z := 1;};

		system_image_transition ::= VkImageMemoryBarrier
		{
			.sType := 45;
			.pNext := zero;
			.srcAccessMask := 0x00000100; // VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT 
			.dstAccessMask := 0x00000800; // VK_ACCESS_TRANSFER_READ_BIT
			.oldLayout := 2; // VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL
			.newLayout := 6; // VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL
			.srcQueueFamilyIndex := ~0;
			.dstQueueFamilyIndex := ~0;
			.image := swapchain.system_image;
			.subresourceRange := VkImageSubresourceRange
			{
				.aspectMask := 0x00000001;
				.baseMipLevel := 0;
				.levelCount := 1;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
		};

		vk.cmd_pipeline_barrier(frame.cmds, 0x00000400, 0x00001000, 0, 0, zero, 0, zero, 1, ref system_image_transition);
		vk.cmd_blit_image(frame.cmds, swapchain.system_image, 6, swapchain_image, 7, 1, ref blit, 0);
	}

	if(will_present)
	{
		barrier.oldLayout = 7;
		barrier.newLayout = 1000001002;
		barrier.srcAccessMask = 0x00001000;
		barrier.dstAccessMask = 0;
		vk.cmd_pipeline_barrier(frame.cmds, 0x00001000, 0x00000001, 0, 0, zero, 0, zero, 1, ref barrier);
	}

	VK_PIPELINE_STAGE_ALL_COMMANDS_BIT ::= 0x00010000;
	vk.cmd_set_event(frame.cmds, graphptr->event, VK_PIPELINE_STAGE_ALL_COMMANDS_BIT);

	vk_check(vk.end_command_buffer(frame.cmds));
	wait_stage : s32 := 0;

	signal_semaphore_count : u32 mut := 1;
	// if we're not presenting then we dont need a signal semaphore
	if(!will_present)
	{
		signal_semaphore_count = 0;
	}
	submit ::= VkSubmitInfo
	{
		.sType := 4;
		.pNext := zero;
		.waitSemaphoreCount := 0;
		.pWaitSemaphores := zero;
		.pWaitDstStageMask := ref wait_stage;
		.commandBufferCount := 1;
		.pCommandBuffers := ref (frame.cmds);
		.signalSemaphoreCount := signal_semaphore_count;
		.pSignalSemaphores := ref (frame.swapchain_sem);
	};
	vk_check(vk.queue_submit(hardware.graphics_queue, 1, ref submit, frame.swapchain_fence));
	vk_check(vk.wait_for_fences(hardware.used_device, 1, ref (frame.swapchain_fence), 1, -1@u64));
	vk_check(vk.reset_fences(hardware.used_device, 1, ref (frame.swapchain_fence)));

	// present if we need to.
	present_res : s32 mut;
	if(will_present)
	{
		present ::= VkPresentInfoKHR
		{
			.sType := 1000001001;
			.pNext := zero;
			.waitSemaphoreCount := 1;
			.pWaitSemaphores := ref (frame.swapchain_sem);
			.swapchainCount := 1;
			.pSwapchains := ref(swapchain.handle);
			.pImageIndices := ref image_index;
			.pResults := ref present_res;
		};
		vk_check(vk.queue_present_khr(hardware.graphics_queue, ref present));
		vk_check(present_res);
	}

	core.current_frame = ((core.current_frame + 1) % 2);
	return ret;
};

impl_gpu_wait ::= func(g : gpu_graph -> v0)
{
	poll_interval_ms ::= 1;

	graphdata ::= graph.data # (g@s64);
	impl_host_wait_event(graphdata->event, poll_interval_ms);
};

graph_entry_type ::= enum
{
	.pass := 0;
	.subgraph := 1;
	.operation := 2;
};

graph_entry ::= struct
{
	handle : u64 mut;
	type : graph_entry_type;
};

graph_resource_dependency_t ::= struct
{
	res : gpu_resource;
	// todo: useful stuff future harry needs.
};

graph_dependency_data_t ::= struct
{
	idx : u64;
	resource_dependencies : graph_resource_dependency_t mut?;
	resource_dependencies_count : u64;
};

graph_data_t ::= struct
{
	info : gpu_graph_info;
	timeline : graph_entry mut?;
	timeline_count : u64;
	timeline_cap : u64;
	dependencies : graph_dependency_data_t mut? mut?;
	dependency_counts : u64 mut?;
	event : u64;

	pass_record : pass_gpu_work_options;
};

// implementation details

impl_graph_will_present ::= func(g : gpu_graph -> bool)
{
	if(core.wsi.type == (gpu_wsi_type.headless))
	{
		// for a headless build, if we have a present pass we just ignore it
		return false;
	}

	graphptr ::= graph.data # (g@s64);
	return (graphptr->info.flags & (gpu_graph_flag.present)) != zero;
};

impl_graph_writes_to_system_image ::= func(g : gpu_graph -> bool)
{
	graphptr ::= graph.data # (g@s64);
	count ::= graphptr->timeline_count;

	counter : u64 mut;
	cur_entry : graph_entry mut;
	cur_pass : gpu_pass mut;
	for(counter = 0, counter < count, counter = counter + 1)
	{
		cur_entry = deref (graphptr->timeline # counter);
		if(cur_entry.type == (graph_entry_type.subgraph))
		{
			if(impl_graph_writes_to_system_image(cur_entry.handle@s64@gpu_graph))
			{
				return true;
			}
		}
		else
		{
			if(cur_entry.type == (graph_entry_type.pass))
			{
				cur_pass = ((cur_entry.handle)@s64@gpu_pass);
				if(impl_pass_writes_to_system_image(cur_pass))
				{
					return true;
				}
			}
			else
			{
				if(impl_operation_writes_to_system_image(cur_entry.handle@s64@gpu_operation))
				{
					return true;
				}
			}
		}
	}
	return false;
};

impl_alloc_new_graph ::= func(a : arena mut? -> graph_data_t mut?)
{
	if(graph.cap == 0)
	{
		graph.data = arena_alloc(a, __sizeof(graph_data_t) * 64);
		graph.cap = 64;
	}
	if(graph.count > graph.cap)
	{
		putzstr("ran out of graphs. todo: fix this.");
		__debugbreak();
	}
	id ::= graph.count;
	graph.count = (graph.count + 1);
	return graph.data # id;
};

impl_record_subgraph_work ::= func(g : gpu_graph, long : arena mut?, short : arena mut? -> gpu_err)
{
	return gpu_err{.code := gpu_err_code.unknown; .msg := "subgraphs are nyi";};
};

impl_wait_dependencies ::= func(g : graph_data_t mut?, idx : u64, short : arena mut? -> v0)
{
	my_entry ::= (g->timeline # idx);
	my_handle ::= my_entry->handle;
	// we're going to iterate over all dependencies
	deps ::= deref(g->dependencies # idx);
	deps_count ::= deref(g->dependency_counts # idx);

	// make an array to store all the events we're gonna wait on
	event_array : u64 mut? := arena_alloc(short, __sizeof(u64) * deps_count);

	// populate event_array
	i : u64 mut;
	for(i = 0, i < deps_count, i = i + 1)
	{
		dep ::= deref(deps # i);
		dep_entry ::= (g->timeline # (dep.idx));
		handle ::= dep_entry->handle;
		if(dep_entry->type == (graph_entry_type.subgraph))
		{
			as_graph ::= deref(graph.data # (handle@s64));
			deref(event_array # i) = (as_graph.event);
		}
		else
		{
			if(dep_entry->type == (graph_entry_type.pass))
			{
				as_pass ::= deref(pass.data # (handle@s64));
				deref(event_array # i) = (as_pass.event);
			}
			else
			{
				as_operation ::= deref(operation.data # (handle@s64));
				deref(event_array # i) = (as_operation.event);
			}
		}
	}

	VK_PIPELINE_STAGE_ALL_COMMANDS_BIT ::= 0x00010000;
	// do vulkan wait
	frame ::= deref(core.frames # (core.current_frame));
	if(deps_count > 0)
	{
		vk.cmd_wait_events(frame.cmds, deps_count@_, event_array, VK_PIPELINE_STAGE_ALL_COMMANDS_BIT, VK_PIPELINE_STAGE_ALL_COMMANDS_BIT, zero, zero, zero, zero, zero, zero);
	}
};

impl_notify_dependencies ::= func(g : graph_data_t mut?, idx : u64 -> v0)
{
	event : u64 mut;

	entry ::= (g->timeline # idx);
	handle ::= entry->handle;
	if(entry->type == (graph_entry_type.subgraph))
	{
		as_graph ::= deref(graph.data # (handle@s64));
		event = (as_graph.event);
	}
	else
	{
		if(entry->type == (graph_entry_type.pass))
		{
			as_pass ::= deref(pass.data # (handle@s64));
			event = (as_pass.event);
		}
		else
		{
			as_operation ::= deref(operation.data # (handle@s64));
			event = (as_operation.event);
		}
	}
	frame ::= deref(core.frames # (core.current_frame));
	VK_PIPELINE_STAGE_ALL_COMMANDS_BIT ::= 0x00010000;
	vk.cmd_set_event(frame.cmds, event, VK_PIPELINE_STAGE_ALL_COMMANDS_BIT);
};

impl_graph_entry_populate_resource_dependencies ::= func(g : gpu_graph, idx : u64 -> v0)
{
	// idx is the graph whose dependency array needs populating
	graphptr ::= graph.data # (g@s64);
	// here's the array and its length
	deps : graph_dependency_data_t mut? := deref(graphptr->dependencies # idx);
	deps_count ::= deref(graphptr->dependency_counts # idx);
	i : u64 mut;
	for(i = 0, i < deps_count, i = i + 1)
	{
		curdep ::= deps # i;
		// curdep->idx is already set
		// but its resource_dependencies and resource_dependencies_count need to be sorted out.
		impl_graph_entry_decipher_dependent_resources(g, idx, curdep->idx, ref(curdep->resource_dependencies), ref(curdep->resource_dependencies_count));
	}
};

impl_graph_entry_decipher_dependent_resources ::= func(g : gpu_graph, our_idx : u64, dep_idx : u64, out_resdeps : graph_resource_dependency_t mut? mut?, out_resdeps_count : u64 mut? -> v0)
{
	graphptr ::= graph.data # (g@s64);

	us ::= graphptr->timeline # our_idx;
	them ::= graphptr->timeline # dep_idx;

	// so 'us' depends on 'them'
	// example: 'them' is a compute pass that writes to a buffer
	// 'us' is a graphics pass that uses that buffer as draw-indirect-count.
	// that means out_resdeps should contain an entry that specifies that the buffer needs a memory barrier:
	//	- compute shader in 'them' sets up a write-via-dispatch
	//	- vertex shader in 'us' needs that write to be resident by the time the indirect command reads it
	// so the vulkan-side memory barrier might look like:
	// VkBufferMemoryBarrier barrier{
    // VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER,
    // nullptr,
    // VK_ACCESS_SHADER_WRITE_BIT,             // src access (compute shader write)
    // VK_ACCESS_INDIRECT_COMMAND_READ_BIT,    // dst access (indirect read)
    // VK_QUEUE_FAMILY_IGNORED,
    // VK_QUEUE_FAMILY_IGNORED,
    // that_buffer.vk_handle,
    // 0,
    // VK_WHOLE_SIZE
	// };

	// you will need to add data members to graph_resource_dependency_t to contain all this necessary information
	// ive only added the res gpu resource for now because thats obvious.
	// then then fill an array of em here into output variables.
	// good luck lmao

	// i reckon this will be alot of code because 'us' and 'them' can be either a subgraph, pass or operation... ugh...
	// good luck darling
	deref(out_resdeps) = zero;
	deref(out_resdeps_count) = zero;
};
