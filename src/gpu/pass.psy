pass : pass_state mut := zero;

pass_state ::= struct
{
	data : pass_data_t mut? mut;
	count : u64 mut;
	cap : u64 mut;
};

// internal pass data.
pass_data_t ::= struct
{
	info : gpu_pass_info;
	metabuf : u64;
	metabuf_size : u64;
	is_compute : bool;
	pipeline : u64;
	event : u64;

	targets_swapchain : bool;
	colour_target_dimensions : u32[2];

	descriptor_sets : u64 mut[2];
};

pass_gpu_work_options ::= struct
{
	previously_cleared_views : u64 mut?;
	previously_cleared_views_count : u64;
	previously_cleared_views_cap : u64;
};

impl_pass_gpu_mark_view_cleared ::= func(image_view : u64, options : pass_gpu_work_options mut?, a : arena mut? -> v0)
{
	if(options->previously_cleared_views_cap == 0)
	{
		options->previously_cleared_views_cap = 8;
		options->previously_cleared_views = arena_alloc(a, __sizeof(u64) * (options->previously_cleared_views_cap));
	}
	while((options->previously_cleared_views_count) >= (options->previously_cleared_views_cap))
	{
		oldcap ::= options->previously_cleared_views_cap;
		olddata ::= options->previously_cleared_views;
		options->previously_cleared_views_cap = (options->previously_cleared_views_cap * 2);
		options->previously_cleared_views = arena_alloc(a, __sizeof(u64) * (options->previously_cleared_views_cap));
		memcopy(options->previously_cleared_views, olddata, __sizeof(u64) * oldcap);
	}
	idx ::= options->previously_cleared_views_count;
	options->previously_cleared_views_count = (options->previously_cleared_views_count + 1);
	deref(options->previously_cleared_views # idx) = image_view;
};

impl_pass_gpu_clear_views ::= func(options : pass_gpu_work_options mut? -> v0)
{
	options->previously_cleared_views_count = 0;
};

impl_pass_gpu_has_view_been_cleared_before ::= func(image_view : u64, options : pass_gpu_work_options? -> bool)
{
	i : u64 mut;
	for(i = 0, i < (options->previously_cleared_views_count), i = i + 1)
	{
		if(deref(options->previously_cleared_views # i) == image_view)
		{
			return true;
		}
	}
	return false;
};

impl_pass_info_deep_copy ::= func(info : gpu_pass_info, long : arena mut? -> gpu_pass_info)
{
	ret : gpu_pass_info mut := info;
	// resources data needs to be deep copied
	resource_data_cpy ::= arena_alloc(long, __sizeof(deref (info.resources_data)) * info.resources_count);
	memcopy(resource_data_cpy, info.resources_data, __sizeof(deref (info.resources_data)) * info.resources_count);
	ret.resources_data = resource_data_cpy;

	if(!impl_shader_is_compute(info.shader))
	{
		// and graphics colour target data
		graphics : gpu_graphics_state mut := info.graphics;
		colour_targets_data_cpy ::= arena_alloc(long, __sizeof(deref (graphics.colour_targets_data)) * graphics.colour_targets_count);
		memcopy(colour_targets_data_cpy, graphics.colour_targets_data, __sizeof(deref (graphics.colour_targets_data)) * graphics.colour_targets_count);
		graphics.colour_targets_data = colour_targets_data_cpy;
		ret.graphics = graphics;
	}
	return ret;
};

impl_gpu_create_pass ::= func(info : gpu_pass_info, long : arena mut?, short : arena mut? -> gpu_pass_result)
{
	retid ::= pass.count;
	passptr ::= impl_alloc_new_pass(long);
	passptr->info = impl_pass_info_deep_copy(info, long);
	// first thing we need is a metabuffer.
	// this is a non-BDA buffer that contains all the BDA addresses.
	// let's figure out how many buffer resources we have
	buffer_rescount : u64 mut := 0;
	counter : u64 mut;
	cur_resource : resource_data_t mut;
	for(counter = 0, counter < (info.resources_count), counter = counter + 1)
	{
		cur_resource = deref (resource.data # (deref(info.resources_data # counter)@s64));
		if(cur_resource.is_buffer)
		{
			buffer_rescount = buffer_rescount + 1;
		}
	}

	size : u64 mut := 1;
	if(buffer_rescount > 0)
	{
		size = buffer_rescount * __sizeof(u64);
	}
	meta_create ::= VkBufferCreateInfo
	{
		.sType := 12;
		.pNext := zero;
		.flags := 0;
		.size := size;
		.usage := (0x00000020 | 0x00000002); // VK_BUFFER_USAGE_STORAGE_BUFFER_BIT | VK_BUFFER_USAGE_TRANSFER_DST_BIT 
		.sharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref(hardware.used_qfi);
	};
	vk_check(vk.create_buffer(hardware.used_device, ref meta_create, zero, ref (passptr->metabuf)));
	(passptr->metabuf_size) = size;
	buffer_alloc_result : gpu_err mut := zero;
	impl_bind_buffer_mem(passptr->metabuf, size, true, ref buffer_alloc_result);
	if(buffer_alloc_result.code != zero)
	{
		return gpu_pass_result
		{
			.res := zero;
			.err := buffer_alloc_result;
		};
	}

	metabuf_prefix ::= "Metabuffer: ";
	prefixlen ::= zstrlen(metabuf_prefix);
	namelen ::= zstrlen(passptr->info.name);
	metabuf_debug_name_str : u8 mut? := arena_alloc(long, prefixlen + namelen);
	memcopy(metabuf_debug_name_str, metabuf_prefix, prefixlen);
	memcopy(metabuf_debug_name_str # prefixlen, passptr->info.name, namelen);

	impl_label_buffer(hardware.used_device, passptr->metabuf, metabuf_debug_name_str);

	passptr->targets_swapchain = false;

	passptr->is_compute = impl_shader_is_compute(info.shader);
	ginfo ::= info.graphics;
	cur_colour_target : gpu_resource mut;
	// create pipeline.
	if(passptr->is_compute)
	{
		passptr->pipeline = impl_create_compute_pipeline(info.shader, info.compute);
	}
	else
	{
		if((ginfo.draw_buffer) != (gpu_resource.invalid))
		{
			drawbuf_handle ::= (ginfo.draw_buffer)@s64;
			drawbuf : resource_data_t? := resource.data # drawbuf_handle;
			dbinfo ::= drawbuf->info;
			dbflags ::= dbinfo.buffer_flags;
			if((dbflags & (gpu_buffer_flag.draw_buffer)) == zero)
			{
				return gpu_pass_result
				{
					.res := gpu_pass.invalid;
					.err := gpu_err
					{
						.code := gpu_err_code.malformed;
						.msg := "error: draw buffer of graphics renderer did not have gpu_buffer_flag.draw_buffer";
					};
				};
			}
		}

		passptr->pipeline = impl_create_graphics_pipeline(info.shader, info.graphics, long);
		for(counter = 0, counter < (ginfo.colour_targets_count), counter = counter + 1)
		{
			cur_colour_target = deref((ginfo.colour_targets_data) # counter);
			if(cur_colour_target == (gpu_resource.window_resource))
			{
				swapchain_err = impl_require_swapchain(long);
				if(swapchain_err.code != zero)
				{
					return gpu_pass_result
					{
						.res := gpu_pass.invalid;
						.err := swapchain_err;
					};
				}
				passptr->targets_swapchain = true;
			}
		}
		swapchain_err : gpu_err mut := zero;

		(passptr->colour_target_dimensions) = impl_get_pass_colour_target_dimensions(passptr);
		if(ginfo.depth_target == (gpu_resource.window_resource))
		{
			swapchain_err = impl_require_swapchain(long);
			if(swapchain_err.code != zero)
			{
				return gpu_pass_result
				{
					.res := gpu_pass.invalid;
					.err := swapchain_err;
				};
			}
			passptr->targets_swapchain = true;
		}

	}

	passptr->event = impl_create_empty_event();

	write_err ::= impl_write_resources(passptr, long, short);
	if(write_err.code != zero)
	{
		return gpu_pass_result
		{
			.res := gpu_pass.invalid;
			.err := descriptor_err;
		};
	}
	descriptor_err ::= impl_populate_descriptors(passptr, long, short);
	if(descriptor_err.code != zero)
	{
		return gpu_pass_result
		{
			.res := gpu_pass.invalid;
			.err := descriptor_err;
		};
	}

	return gpu_pass_result
	{
		.res := retid@gpu_pass;
		.err := zero;
	};
};

impl_alloc_new_pass ::= func(a : arena mut? -> pass_data_t mut?)
{
	if(pass.cap == 0)
	{
		pass.data = arena_alloc(a, __sizeof(pass_data_t) * 32);
		pass.cap = 32;
	}
	if(pass.count > pass.cap)
	{
		putzstr("ran out of pass capacity. todo: fix this.");
		__debugbreak();
	}
	id ::= pass.count;
	pass.count = (pass.count + 1);
	return pass.data # id;
};

impl_pass_writes_to_system_image ::= func(p : gpu_pass -> bool)
{
	passptr ::= (pass.data # (p@s64));
	passinfo ::= passptr->info;
	ginfo ::= passinfo.graphics;
	counter : u64 mut := 0;
	cur_colour_target : gpu_resource mut;
	if(!(passptr->is_compute))
	{
		for(counter = 0, counter < (ginfo.colour_targets_count), counter = counter + 1)
		{
			cur_colour_target = deref((ginfo.colour_targets_data) # counter);
			if(cur_colour_target == (gpu_resource.window_resource))
			{
				return true;
			}
		}
	}
	return false;
};

impl_get_pass_colour_target_dimensions ::= func(passptr : pass_data_t? -> u32[2])
{
	pinfo ::= passptr->info;
	ginfo ::= pinfo.graphics;
	if(ginfo.colour_targets_count == 0)
	{
		putzstr("bad pass - no colour targets.");
	}
	first_colour_target : gpu_resource mut := zero;
	first_colour_target = deref ((ginfo.colour_targets_data) # 0);
	//first_colour_target ::= deref ((ginfo.colour_targets_data) # 0);
	if(first_colour_target == (gpu_resource.window_resource))
	{
		return u32[2]
		{
			swapchain.width@_;
			swapchain.height@_;
		};
	}
	res ::= (resource.data # (first_colour_target@s64));
	resinfo ::= res->info;
	return resinfo.image_dimensions;
};


impl_record_compute_work ::= func(passptr : pass_data_t mut?, frame_id : u64 -> gpu_err)
{
	frame ::= deref(core.frames # frame_id);
	pinfo ::= passptr->info;
	cinfo ::= pinfo.compute;

	vk.cmd_bind_pipeline(frame.cmds, 1, passptr->pipeline);
	vk.cmd_bind_descriptor_sets(frame.cmds, 1, hardware.pipeline_layout, 0, 1, passptr->descriptor_sets # (core.current_frame), 0, zero);
	vk.cmd_dispatch(frame.cmds, cinfo.kernelx@_, cinfo.kernely@_, cinfo.kernelz@_);
	return zero;
};

impl_record_graphics_work ::= func(passptr : pass_data_t mut?, options : pass_gpu_work_options mut?, frame_id : u64, long : arena mut?, short : arena mut? -> gpu_err)
{
	frame ::= deref(core.frames # frame_id);
	pinfo ::= passptr->info;
	ginfo ::= pinfo.graphics;
	ccount ::= ginfo.colour_targets_count;

	if(ccount == 0)
	{
		return gpu_err
		{
			.code := gpu_err_code.malformed;
			.msg := "doesn't make sense to have a graphics pass with no colour attachments. please use one.";
		};
	}

	colour_attachments : VkRenderingAttachmentInfo mut? := arena_alloc(short, __sizeof(VkRenderingAttachmentInfo) * ccount);
	colour_transitions : VkImageMemoryBarrier mut? := arena_alloc(short, __sizeof(VkImageMemoryBarrier) * ccount);
	colour_transition_count : u64 mut := 0;

	counter : u64 mut := 0;
	colour_target : gpu_resource mut;
	resptr : resource_data_t mut? mut;
	render_target : u64 mut;
	render_target_view : u64 mut;
	depth_target : u64 mut;
	depth_target_view : u64 mut;

	if(passptr->targets_swapchain)
	{
		swapchain_err ::= impl_require_swapchain(long);
		if(swapchain_err.code != zero)
		{
			return swapchain_err;
		}

		dims ::= passptr->colour_target_dimensions;
		if(deref(dims # 0) != (swapchain.width@_))
		{
			(passptr->colour_target_dimensions) = impl_get_pass_colour_target_dimensions(passptr);
		}
		if(deref(dims # 1) != (swapchain.height@_))
		{
			(passptr->colour_target_dimensions) = impl_get_pass_colour_target_dimensions(passptr);
		}
	}
	dimensions ::= (passptr->colour_target_dimensions);
	w ::= deref(dimensions # 0);
	h ::= deref(dimensions # 1);

	for(counter = 0, counter < ccount, counter = counter + 1)
	{
		colour_target = deref ((ginfo.colour_targets_data) # counter);
		render_target = 0;
		render_target_view = 0;

		if(colour_target == (gpu_resource.window_resource))
		{
			// need a new swapchain!
			render_target = (swapchain.system_image);
			render_target_view = (swapchain.system_image_view);
		}
		else
		{
			resptr = (resource.data # (colour_target@s64));
			render_target = (resptr->vk_handle);
			render_target_view = (resptr->image_view);
		}

		// we have a colour output attachment
		// we need to know if its been cleared already
		// if not then we are the first and should do it (+ the transition)
		// otherwise just load and assume there is no race condition
		already_cleared ::= impl_pass_gpu_has_view_been_cleared_before(render_target_view, options);

		rtv_load_op : s32 mut := 1; // VK_ATTACHMENT_LOAD_OP_CLEAR;
		if(already_cleared)
		{
			rtv_load_op = 0; // VK_ATTACHMENT_LOAD_OP_LOAD
		}
		else
		{
			impl_pass_gpu_mark_view_cleared(render_target_view, options, long);
		}

		deref(colour_attachments # counter) = VkRenderingAttachmentInfo
		{
			.sType := 1000044001;
			.pNext := zero;
			.imageView := render_target_view;
			.imageLayout := 2; //VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL
			.resolveMode := 0;
			.resolveImageView := 0;
			.resolveImageLayout := 0;
			.loadOp := rtv_load_op;
			.storeOp := 0;
			.clearValue := VkClearValue{.color := VkClearColorValue{.float32 := ginfo.clear_colour;};};
		};
		if(!already_cleared)
		{
			deref (colour_transitions # colour_transition_count) = VkImageMemoryBarrier
			{
				.sType := 45;
				.pNext := zero;
				.srcAccessMask := 0;
				.dstAccessMask := 0x00000100; // VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT
				.oldLayout := 0;
				.newLayout := 2; // VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL
				.srcQueueFamilyIndex := ~0;
				.dstQueueFamilyIndex := ~0;
				.image := render_target;
				.subresourceRange := VkImageSubresourceRange
				{
					.aspectMask := 0x00000001; // VK_IMAGE_ASPECT_COLOR_BIT 
					.baseMipLevel := 0;
					.levelCount := 1;
					.baseArrayLayer := 0;
					.layerCount := 1;
				};
			};
			colour_transition_count = colour_transition_count + 1;
		}
	}
	depth_attachment_value : VkRenderingAttachmentInfo mut;
	depth_attachment : VkRenderingAttachmentInfo? mut := zero;
	if(ginfo.depth_target != (gpu_resource.invalid))
	{
		depth_attachment = ref depth_attachment_value;
		if(ginfo.depth_target == (gpu_resource.window_resource))
		{
			depth_target = (swapchain.system_depth_image);
			depth_target_view = (swapchain.system_depth_image_view);
		}
		else
		{
			// its an actual target
			depthresptr ::= (resource.data # (ginfo.depth_target@s64));
			depth_target = (depthresptr->vk_handle);
			depth_target_view = (depthresptr->image_view);
		}

		depth_already_cleared ::= impl_pass_gpu_has_view_been_cleared_before(depth_target_view, options);

		dtv_load_op : s32 mut := 1; // VK_ATTACHMENT_LOAD_OP_CLEAR;
		if(depth_already_cleared)
		{
			dtv_load_op = 0; // VK_ATTACHMENT_LOAD_OP_LOAD
		}
		else
		{
			impl_pass_gpu_mark_view_cleared(depth_target_view, options, long);
		}

		depth_attachment_value = VkRenderingAttachmentInfo
		{
			.sType := 1000044001;
			.pNext := zero;
			.imageView := depth_target_view;
			.imageLayout := 1000241000; //VK_IMAGE_LAYOUT_DEPTH_ATTACHMENT_OPTIMAL
			.resolveMode := 0;
			.resolveImageView := 0;
			.resolveImageLayout := 0;
			.loadOp := dtv_load_op;
			.storeOp := 0;
			.clearValue := VkClearValue{.color := VkClearColorValue{.float32 := f32[4]{1.0; 0.0; 0.0; 0.0;};};};
		};
	}
	// if we have any colour transitions # all, do a pipeline barrier with them.
	if(colour_transition_count > 0)
	{
		// BOTTOM_OF_PIPE
		// COLOUR_ATTACHMENT_OUTPUT
		vk.cmd_pipeline_barrier(frame.cmds, 0x00002000, 0x00000400, 0, 0, zero, 0, zero, colour_transition_count@_, colour_transitions);
	}

	render ::= VkRenderingInfo
	{
		.sType := 1000044000;
		.pNext := zero;
		.flags := 0;
		.renderArea := VkRect2D
		{
			.offset := VkOffset2D{.x := 0; .y := 0;};
			.extent := VkExtent2D
			{
				.width := w;
				.height := h;
			};
		};
		.layerCount := 1;
		.viewMask := 0;
		.colorAttachmentCount := ccount@_;
		.pColorAttachments := colour_attachments;
		.pDepthAttachment := depth_attachment;
		.pStencilAttachment := zero;
	};
	vk.cmd_begin_rendering(frame.cmds, ref render);
	// actually do rendering.
	vk.cmd_bind_pipeline(frame.cmds, 0, passptr->pipeline);
	// todo: bind index buffer if it exists.
	vk.cmd_bind_descriptor_sets(frame.cmds, 0, hardware.pipeline_layout, 0, 1, passptr->descriptor_sets # (core.current_frame), 0, zero);
	viewport ::= VkViewport
	{
		.x := 0.0;
		.y := h@_;
		.width := w@_;
		.height := -1.0 * h@_;
		.minDepth := 0.0;
		.maxDepth := 1.0;
	};
	vk.cmd_set_viewport(frame.cmds, 0, 1, ref viewport);

	scissor ::= VkRect2D
	{
		.offset := VkOffset2D
		{
			.x := 0;
			.y := 0;
		};
		.extent := VkExtent2D
		{
			.width := w;
			.height := h;
		};
	};
	vk.cmd_set_scissor(frame.cmds, 0, 1, ref scissor);

	drawbufres : resource_data_t? mut;
	drawbufinfo : gpu_resource_info mut;
	// todo: draw buffer logic.
	// for now we just do the draw.
	tri_count ::= ginfo.static_tri_count;
	drawbuf ::= ginfo.draw_buffer;
	idxbuf ::= ginfo.index_buffer;
	if(idxbuf == (gpu_resource.invalid))
	{
		if(drawbuf == (gpu_resource.invalid))
		{
			vk.cmd_draw(frame.cmds, (tri_count * 3)@_, 1, 0, 0);
		}
		else
		{
			drawbufres = (resource.data # (drawbuf@s64));
			drawbufinfo = drawbufres->info;
			draw_buf_max_size_unindexed ::= ((drawbufinfo.data_size) - __sizeof(u32)) / __sizeof(gpu_draw_command);
			vk.cmd_draw_indirect_count(frame.cmds, drawbufres->vk_handle, __sizeof(u32), drawbufres->vk_handle, 0, draw_buf_max_size_unindexed@_, __sizeof(gpu_draw_command));
		}
	}
	if(idxbuf != (gpu_resource.invalid))
	{
		idxbufres ::= resource.data # (idxbuf@s64);
		idxbufinfo ::= idxbufres->info;
		vk.cmd_bind_index_buffer(frame.cmds, idxbufres->vk_handle, 0, 1);
		if(drawbuf == (gpu_resource.invalid))
		{
			vk.cmd_draw_indexed(frame.cmds, (tri_count * 3)@_, 1, 0, 0, 0);
		}
		else
		{
			drawbufres = (resource.data # (drawbuf@s64));
			drawbufinfo = drawbufres->info;
			draw_buf_max_size_indexed ::= ((drawbufinfo.data_size) - __sizeof(u32)) / __sizeof(gpu_draw_indexed_command);
			vk.cmd_draw_indexed_indirect_count(frame.cmds, drawbufres->vk_handle, __sizeof(u32), drawbufres->vk_handle, 0, draw_buf_max_size_indexed@_, __sizeof(gpu_draw_indexed_command));
		}
	}

	vk.cmd_end_rendering(frame.cmds);
	return zero;
};

impl_record_gpu_work ::= func(p : gpu_pass, options : pass_gpu_work_options mut?, frame_id : u64, long : arena mut?, short : arena mut? -> gpu_err)
{
	passptr : pass_data_t mut? := pass.data # (p@s64);

	frame ::= deref(core.frames # frame_id);
	impl_begin_label(frame.cmds, passptr->info.name);
	ret : gpu_err mut := zero;
	if(passptr->is_compute)
	{
		ret = impl_record_compute_work(passptr, frame_id);
	}
	else
	{
		ret = impl_record_graphics_work(passptr, options, frame_id, long, short);
	}
	impl_end_label(frame.cmds);
	return ret;
};

impl_pass_uses_resource ::= func(p : gpu_pass, res : gpu_resource -> bool)
{
	counter : u64 mut;
	cur_res : gpu_resource mut;
	passptr : pass_data_t? := pass.data # (p@s64);
	passinfo ::= passptr->info;
	cur_residx : u64 mut;
	for(cur_residx = 0, cur_residx < (passinfo.resources_count), cur_residx = cur_residx + 1)
	{
		cur_res = deref((passinfo.resources_data) # cur_residx);
		if(cur_res == res)
		{
			return true;
		}
	}
	return false;
};

impl_create_empty_event ::= func(-> u64)
{
	create ::= VkEventCreateInfo
	{
		.sType := 10;
		.pNext := zero;
		// note: if we want to remove the ability to have the host wait (i.e delete impl_host_wait_event and gpu_wait) then pass VK_EVENT_CREATE_DEVICE_ONLY_BIT
		.flags := zero;
	};
	evt : u64 mut;
	vk.create_event(hardware.used_device, ref create, zero, ref evt);
	return evt;
};
