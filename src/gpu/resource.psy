resource : resource_state mut := zero;

resource_state ::= struct
{
	data : resource_data_t mut?;
	count : u64;
	cap : u64;
};

resource_data_t ::= struct
{
	info : gpu_resource_info;
	is_buffer : bool;
	vk_handle : u64;
	image_view : u64;
	sampler : u64;
	buffer_device_address : u64;
	mapped_ptr : v0 mut? weak;
};

impl_gpu_create_buffer ::= func(info : gpu_resource_info, a : arena mut? -> gpu_resource_result)
{
	retid ::= resource.count@u64 mut;
	resptr ::= impl_alloc_new_resource(a);
	err ::= impl_init_buffer(resptr, info, a);
	if(err.code != zero)
	{
		retid = -1;
	}
	return gpu_resource_result
	{
		.res := retid@gpu_resource;
		.err := err;
	};
};

impl_gpu_create_image ::= func(info : gpu_resource_info, a : arena mut? -> gpu_resource_result)
{
	retid ::= resource.count@u64 mut;
	resptr ::= impl_alloc_new_resource(a);
	err ::= impl_init_image(resptr, info, a);
	if(err.code != zero)
	{
		retid = -1;
	}
	return gpu_resource_result
	{
		.res := retid@gpu_resource;
		.err := err;
	};
};

impl_gpu_resource_write ::= func(res : gpu_resource, data : v0? weak, data_size : u64, offset : u64 -> gpu_err)
{
	resptr ::= (resource.data # (res@s64));
	rinfo ::= ref (resptr->info);

	memcopy(rinfo->data@u8 mut? # offset, data, data_size);
	return impl_write_a_resource(resptr, data_size, offset);
};

impl_gpu_resource_read ::= func(res : gpu_resource, buf : v0? weak, len : u64, offset : u64 -> gpu_err)
{
	recorded_len ::= gpu_resource_size(res);
	if(offset >= recorded_len)
	{
		return gpu_err
		{
			.code := gpu_err_code.malformed;
			.msg := "offset was >= the length of the resource";
		};
	}
	resptr ::= resource.data # (res@s64);
	rinfo ::= resptr->info;
	is_dynamic_buffer ::= (((rinfo.buffer_flags) & (gpu_buffer_flag.dynamic)) != zero) && resptr->is_buffer;
	if(is_dynamic_buffer)
	{
		mapped_ptr ::= (resptr->mapped_ptr)@u8 mut? # offset;
		memcopy(buf, mapped_ptr, len);
	}
	else
	{
		ptr ::= (rinfo.data)@u8 mut? # offset;
		memcopy(buf, ptr, len);
	}
	return zero;
};

impl_gpu_resource_size ::= func(res : gpu_resource -> u64)
{
	resptr ::= resource.data # (res@s64);
	rinfo ::= resptr->info;
	return rinfo.data_size;
};

impl_gpu_resource_pointer ::= func(res : gpu_resource -> gpu_resource_pointer_result)
{
	resptr ::= resource.data # (res@s64);
	rinfo ::= resptr->info;
	is_dynamic_buffer ::= (((rinfo.buffer_flags) & (gpu_buffer_flag.dynamic)) != zero) && resptr->is_buffer;
	err : gpu_err mut := zero;
	if(!is_dynamic_buffer)
	{
		err = gpu_err
		{
			.code := gpu_err_code.malformed;
			.msg := "gpu_resource_pointer only useable on dynamic buffers";
		};
	}
	return gpu_resource_pointer_result
	{
		.ptr := resptr->mapped_ptr@_;
		.err := err;
	};
};

impl_gpu_map_resource ::= func(res : gpu_resource -> gpu_map_resource_result)
{
	pointer_result ::= impl_gpu_resource_pointer(res);
	if(pointer_result.err.code != zero)
	{
		return gpu_map_resource_result
		{
			.mapping := zero;
			.err := pointer_result.err;
		};
	}
	return gpu_map_resource_result
	{
		.mapping := arena_create_preallocated(pointer_result.ptr, impl_gpu_resource_size(res));
		.err := zero;
	};
};

impl_gpu_resize_buffer ::= func(res : gpu_resource, new_size : u64, long : arena mut?, short : arena mut? -> v0)
{
	resptr ::= resource.data # (res@s64);
	oldinfo ::= resptr->info;
	newinfo : gpu_resource_info mut := oldinfo;
	if((oldinfo.data_size) == new_size)
	{
		return;
	}

	vk.device_wait_idle(hardware.used_device);
	vk.destroy_buffer(hardware.used_device, resptr->vk_handle, zero);

	if(new_size > (oldinfo.data_size))
	{
		// ok a bit of trouble. old data is too small. need to realloc to be large enough.
		newinfo.data = arena_alloc(long, new_size);
		newinfo.data_size = new_size;

		// preserve old data. remember - just because it was zero'd initially doesnt mean its zero'd now
		memcopy(newinfo.data, oldinfo.data, oldinfo.data_size);
		// fill the rest with zeros.
		offset_ptr ::= (newinfo.data)@u8 mut? mut;
		offset_ptr = (offset_ptr # (oldinfo.data_size));
		memzero(offset_ptr, new_size - (oldinfo.data_size));
	}

	impl_init_buffer(resptr, newinfo, long);

	// problem is, all the passes that use this buffer as a resource will need to have their metabuffer written to.
	// this is because the metabuffer will contain the BDA of the old buffer which is now out of date.
	// solution:
	// for all passes, if they contained this buffer, rewrite their metabuffers.
	counter : u64 mut;
	for(counter = 0, counter < (pass.count), counter = counter + 1)
	{
		if(impl_pass_uses_resource(counter@s64@gpu_pass, res))
		{
			write_err ::= impl_write_resources(pass.data # counter, long, short);
			// todo: pass this error upwards
		}
	}
};

impl_gpu_resize_image ::= func(res : gpu_resource, new_dimensions : u32[2], long : arena mut?, short : arena mut? -> v0)
{
	resptr ::= resource.data # (res@s64);
	oldinfo ::= resptr->info;
	olddata ::= oldinfo.data;
	olddims ::= oldinfo.image_dimensions;

	// if dimensions match dont do anything
	if((deref(new_dimensions # 0) == deref(olddims # 0)) && (deref(new_dimensions # 1) == deref(olddims # 1)))
	{
		return;
	}

	// if the dimensions change then the data currently sitting in the resource really doesnt make any sense anymore
	// let's clear it all out
	resptr->info.data = zero;
	resptr->info.image_dimensions = new_dimensions;
	resptr->info.data_size = impl_naive_image_size(resptr->info);
	resptr->info.flags = (resptr->info.flags | (gpu_resource_flag.zero_memory));

	vk.device_wait_idle(hardware.used_device);
	vk.destroy_image(hardware.used_device, resptr->vk_handle, zero);
	vk.destroy_image_view(hardware.used_device, resptr->image_view, zero);
	vk.destroy_sampler(hardware.used_device, resptr->sampler, zero);

	impl_init_image(resptr, resptr->info, long);

	// problem is, all the passes that use this image as a resource will need to have their descriptors rewritten.
	// solution:
	// for all passes, if they contained this image, rewrite their resources + repopulate their descriptors.
	counter : u64 mut;
	for(counter = 0, counter < (pass.count), counter = counter + 1)
	{
		if(impl_pass_uses_resource(counter@s64@gpu_pass, res))
		{
			write_err ::= impl_write_resources(pass.data # counter, long, short);
			// todo: pass this error upwards
			impl_update_descriptors(pass.data # counter, long, short);
		}
	}
};

impl_cpu_gpu_transfer ::= func(dst_gpu_res : u64, dst_gpu_is_buffer : bool, src_data : u8? weak, src_data_size : u64, image_dimensions : u32[2], dst_offset : u64, is_depth_image : bool -> gpu_err)
{
	vk.device_wait_idle(hardware.used_device);
	impl_begin_scratch_commands();
	staging_create ::= VkBufferCreateInfo
	{
		.sType := 12;
		.pNext := zero;
		.flags := 0;
		.size := src_data_size;
		.usage := 0x00000001; //VK_BUFFER_USAGE_TRANSFER_SRC_BIT
		.sharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref(hardware.used_qfi);
	};
	staging : u64 mut;
	vk.create_buffer(hardware.used_device, ref staging_create, zero, ref staging);
	// todo: write to the buffer using map memory.
	old_cpumem ::= core.vallocator.buffer_cpu;
	buffer_alloc_result : gpu_err mut := zero;
	cursor ::= impl_bind_buffer_mem(staging, src_data_size, false, ref buffer_alloc_result);
	if(buffer_alloc_result.code != zero)
	{
		return buffer_alloc_result;
	}
	cpumem ::= core.vallocator.buffer_cpu;
	mapped_ptr ::= (cpumem.mapped_ptr)@u8 mut?;
	memcopy(mapped_ptr # cursor, src_data # dst_offset, src_data_size);

	aspect_mask : s32 mut := 1;
	if(is_depth_image)
	{
		aspect_mask = 2;
	}

	if(dst_gpu_is_buffer)
	{
		bufcpy ::= VkBufferCopy
		{
			.srcOffset := 0;
			.dstOffset := dst_offset@_;
			.size := src_data_size;
		};
		vk.cmd_copy_buffer(core.scratch.cmds, staging, dst_gpu_res, 1, ref bufcpy);
	}
	else
	{
		imgcpy ::= VkBufferImageCopy
		{
			.bufferOffset := 0;
			.bufferRowLength := 0;
			.bufferImageHeight := 0;
			.imageSubresource := VkImageSubresourceLayers
			{
				.aspectMask := aspect_mask;
				.mipLevel := 0;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
			.imageOffset := VkOffset3D{.x := 0; .y := 0; .z := 0;};
			.imageExtent := VkExtent3D{.width := deref(image_dimensions # 0); .height := deref(image_dimensions # 1); .depth := 1;};
		};
		barrier ::= VkImageMemoryBarrier
		{
			.sType := 45;
			.pNext := zero;
			.srcAccessMask := 0;
			.dstAccessMask := 0x00001000;//VK_ACCESS_TRANSFER_WRITE_BIT
			.oldLayout := 0;
			.newLayout := 7; // VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL
			.srcQueueFamilyIndex := ~0;
			.dstQueueFamilyIndex := ~0;
			.image := dst_gpu_res;
			.subresourceRange := VkImageSubresourceRange
			{
				.aspectMask := aspect_mask;
				.baseMipLevel := 0;
				.levelCount := 1;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
		};
		vk.cmd_pipeline_barrier(core.scratch.cmds, 0, 0x00001000, 0, 0, zero, 0, zero, 1, ref barrier);
		vk.cmd_copy_buffer_to_image(core.scratch.cmds, staging, dst_gpu_res, 7, 1, ref imgcpy);
	}

	impl_end_and_execute_scratch_commands();
	vk.destroy_buffer(hardware.used_device, staging, zero);
	return zero;
};

impl_write_a_resource ::= func(resptr : resource_data_t?, size : u64, offset : u64 -> gpu_err)
{
	rinfo ::= resptr->info;
	is_dynamic_buffer ::= (((rinfo.buffer_flags) & (gpu_buffer_flag.dynamic)) != zero) && resptr->is_buffer;
	if(is_dynamic_buffer)
	{
		mapped_ptr ::= (resptr->mapped_ptr)@u8 mut?;
		memcopy(mapped_ptr#offset, rinfo.data, size);
		return zero;
	}
	is_depth_image ::= ((rinfo.image_type) == (gpu_image_type.depth));
	return impl_cpu_gpu_transfer(resptr->vk_handle, resptr->is_buffer, rinfo.data, size, rinfo.image_dimensions, offset, is_depth_image);
};

impl_write_resources ::= func(passptr : pass_data_t mut?, long : arena mut?, short : arena mut? -> gpu_err)
{
	pinfo ::= passptr->info;
	if((pinfo.resources_count) == 0)
	{
		return zero;
	}

	buffer_addresses : u64 mut? := arena_alloc(short, passptr->metabuf_size);
	bufcount : u64 mut := 0;

	write_err : gpu_err mut := zero;

	i : u64 mut := 0;
	curres : gpu_resource mut;
	resdata : resource_data_t mut;
	for(i = 0, i < (pinfo.resources_count), i = i + 1)
	{
		curres = deref((pinfo.resources_data) # i);
		if(curres != (gpu_resource.invalid))
		{
			if(curres != (gpu_resource.window_resource))
			{
				resdata = deref (resource.data # (curres@s64));
				if(resdata.is_buffer)
				{
					defer bufcount = bufcount + 1;
					deref(buffer_addresses # bufcount) = (resdata.buffer_device_address);
				}
				else
				{
					resinfo ::= resdata.info;
					is_depth_image ::= ((resinfo.image_type) == (gpu_image_type.depth));
					write_err = impl_cpu_gpu_transfer(resdata.vk_handle, false, resdata.info.data, resdata.info.data_size, resdata.info.image_dimensions, 0, is_depth_image);
					if(write_err.code != zero)
					{
						return write_err;
					}
				}
			}
		}
	}

	if(bufcount != ((passptr->metabuf_size) / __sizeof(u64)))
	{
		return gpu_err
		{
			.code := gpu_err_code.unknown;
			.msg := "internal pass metabuffer was of incorrect size. please report this as a bug";
		};
	}

	// write to metabuffer.
	if(bufcount > 0)
	{
		write_err = impl_cpu_gpu_transfer(passptr->metabuf, true, buffer_addresses, passptr->metabuf_size, zero, 0, false);
		if(write_err.code != zero)
		{
			return write_err;
		}
	}
	return zero;
};

impl_init_buffer ::= func(resptr : resource_data_t mut?, info : gpu_resource_info, a : arena mut? -> gpu_err)
{
	resptr->info = info;
	resptr->is_buffer = true;
	create : VkBufferCreateInfo mut := VkBufferCreateInfo
	{
		.sType := 12;
		.pNext := zero;
		.flags := 0;
		.size := info.data_size;
		.usage := (0x00000020 | 0x00020000);
		.sharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref(hardware.used_qfi);
	};
	dynamic ::= ((info.buffer_flags) & (gpu_buffer_flag.dynamic)) != zero;
	if(!dynamic)
	{
		create.usage = (create.usage) | 0x00000002;
	}
	if(((info.buffer_flags) & (gpu_buffer_flag.index_buffer)) != zero)
	{
		create.usage = (create.usage) | 0x00000040;
	}
	if(((info.buffer_flags) & (gpu_buffer_flag.indirect_buffer)) != zero)
	{
		create.usage = (create.usage) | 0x00000100;
	}
	vk_check(vk.create_buffer(hardware.used_device, ref create, zero, ref (resptr->vk_handle)));
	bda ::= VkBufferDeviceAddressInfo
	{
		.sType := 1000244001;
		.pNext := zero;
		.buffer := resptr->vk_handle;
	};
	buffer_alloc_result : gpu_err mut := zero;
	cursor ::= impl_bind_buffer_mem(resptr->vk_handle, info.data_size, !dynamic, ref buffer_alloc_result);
	if(buffer_alloc_result.code != zero)
	{
		return buffer_alloc_result;
	}
	if(dynamic)
	{
		cpumem ::= core.vallocator.buffer_cpu;
		mapped_ptr ::= (cpumem.mapped_ptr)@u8 mut?;
		(resptr->mapped_ptr) = (mapped_ptr # cursor)@_;
	}
	tmp_bda ::= vk.get_buffer_device_address(hardware.used_device, ref bda);
	(resptr->buffer_device_address) = tmp_bda;

	// make a copy of the resource data using the arena.
	resinfo ::= ref(resptr->info);
	resinfo->data = arena_alloc(a, info.data_size);
	if(info.data != zero)
	{
		memcopy(resinfo->data, info.data, info.data_size);
	}

	if(((info.flags) & (gpu_resource_flag.zero_memory)) != zero)
	{
		memfill(resinfo->data, 0, info.data_size);
		// now remove the zero_memory flag from now on as it only applies now
		flagval : s64 mut := resinfo->flags@s64;
		flagval = (flagval & (~(gpu_resource_flag.zero_memory@s64)));
		resinfo->flags = (flagval@_);
	}

	// write resource data to gpu memory.
	write_result ::= impl_write_a_resource(resptr, info.data_size, 0);
	if(write_result.code != zero)
	{
		return write_result;
	}

	// debug utils naming
	if(info.name != zero)
	{
		impl_label_buffer(hardware.used_device, resptr->vk_handle, info.name);
	}
	return zero;
};

impl_init_image ::= func(resptr : resource_data_t mut?, info : gpu_resource_info, a : arena mut? -> gpu_err)
{
	resptr->info = info;
	resptr->is_buffer = false;
	fmt ::= impl_image_type_pixel_format(info.image_type);
	aspect_mask : s32 mut := 0x00000001;
	if((info.image_type) == (gpu_image_type.depth))
	{
		aspect_mask = 0x00000002;
	}
	dims ::= info.image_dimensions;
	w ::= deref(dims # 0);
	h ::= deref(dims # 1);
	create : VkImageCreateInfo mut := VkImageCreateInfo
	{
		.sType := 14;
		.pNext := zero;
		.flags := 0;
		.imageType := 1;
		.format := fmt;
		.extent := VkExtent3D
		{
			.width := w;
			.height := h;
			.depth := 1;
		};
		.mipLevels := 1;
		.arrayLayers := 1;
		.samples := 1;
		.tiling := 0;
		.usage := (0x00000004 | 0x00000002); // VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT 
		.sharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref(hardware.used_qfi);
		.initialLayout := 0;
	};
	if((info.image_flags & (gpu_image_flag.colour_attachment)) != zero)
	{
		create.usage = (create.usage | 0x00000010); // VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT
	}
	if((info.image_flags & (gpu_image_flag.depth_attachment)) != zero)
	{
		create.usage = (create.usage | 0x00000020); // VK_IMAGE_USAGE_DEPTH_STENCIL_ATTACHMENT_BIT
	}
	vk_check(vk.create_image(hardware.used_device, ref create, zero, ref (resptr->vk_handle)));
	impl_bind_image_mem(resptr->vk_handle);

	view_create ::= VkImageViewCreateInfo
	{
		.sType := 15;
		.pNext := zero;
		.flags := 0;
		.image := resptr->vk_handle;
		.viewType := 1;
		.format := fmt;
		.components := VkComponentMapping
		{
			.r := 0;
			.g := 0;
			.b := 0;
			.a := 0;
		};
		.subresourceRange := VkImageSubresourceRange
		{
			.aspectMask := aspect_mask;
			.baseMipLevel := 0;
			.levelCount := 1;
			.baseArrayLayer := 0;
			.layerCount := 1;
		};
	};

	vk_check(vk.create_image_view(hardware.used_device, ref view_create, zero, ref (resptr->image_view)));

	sampler_create ::= VkSamplerCreateInfo
	{
		.sType := 31;
		.pNext := zero;
		.flags := 0;
		.magFilter := 0;
		.minFilter := 0;
		.mipmapMode := 0;
		.addressModeU := 2;
		.addressModeV := 2;
		.addressModeW := 2;
		.mipLodBias := 0.0;
		.anisotropyEnable := 0;
		.maxAnisotropy := 0.0;
		.compareEnable := 0;
		.compareOp := 7;
		.minLod := 0.0;
		.maxLod := 0.0;
		.borderColor := 3;
		.unnormalizedCoordinates := 0;
	};
	vk_check(vk.create_sampler(hardware.used_device, ref sampler_create, zero, ref (resptr->sampler)));

	// make a copy of the resource data using the arena.
	// we want this resource data to be enough for the whole image (as if it was not optimal tiling) for potential transfer purposes later on
	// and the data_size provided doesn't necessarily need to be that.
	// so we calculate that now
	resinfo ::= ref(resptr->info);
	resinfo->data_size = impl_naive_image_size(info);
	resinfo->data = arena_alloc(a, resinfo->data_size);
	// if we were given data to copy then copy that amount. if that doesnt cover the whole image data then the remainder will be random data
	if(info.data != zero)
	{
		memcopy(resinfo->data, info.data, info.data_size);
	}

	// if zero_memory was passed then lets make sure we zero the whole thing
	if(((info.flags) & (gpu_resource_flag.zero_memory)) != zero)
	{
		memfill(resinfo->data, 0, resinfo->data_size);
	}

	// write resource data to gpu memory. yes the whole thing
	write_result ::= impl_write_a_resource(resptr, resinfo->data_size, 0);
	if(write_result.code != zero)
	{
		return write_result;
	}

	// debug utils naming
	if(info.name != zero)
	{
		impl_label_image(hardware.used_device, resptr->vk_handle, info.name);

		view_prefix ::= "View: ";
		prefixlen ::= zstrlen(view_prefix);
		namelen ::= zstrlen(info.name);
		view_debug_name_str : u8 mut? := arena_alloc(a, prefixlen + namelen);
		memcopy(view_debug_name_str, view_prefix, prefixlen);
		memcopy(view_debug_name_str # prefixlen, info.name, namelen);
		impl_label_view(hardware.used_device, resptr->image_view, view_debug_name_str);
	}
	return zero;
};

impl_alloc_new_resource ::= func(a : arena mut? -> resource_data_t mut?)
{
	if(resource.cap == 0)
	{
		resource.data = arena_alloc(a, __sizeof(resource_data_t) * 1024);
		resource.cap = 1024;
	}
	if(resource.count >= (resource.cap))
	{
		olddata ::= resource.data;
		oldcap ::= resource.cap;
		resource.cap = (resource.cap * 2);
		resource.data = arena_alloc(a, __sizeof(resource_data_t) * (resource.cap));
		memcopy(resource.data, olddata, __sizeof(resource_data_t) * oldcap);
	}
	id ::= resource.count;
	resource.count = (resource.count + 1);
	return resource.data # id;
};

impl_image_type_pixel_size ::= func(t : gpu_image_type -> u64)
{
	arr ::= u64[3]
	{
		4; // .rgba => rgba8
		8; // .rgba_float => rgba16f
		4; // .depth => depth32
	};
	return deref(arr # (t@s64));
};

impl_image_type_pixel_format ::= func(t : gpu_image_type -> s32)
{
	arr ::= s32[3]
	{
		37;  // VK_FORMAT_R8G8B8A8_UNORM
		97;  // VK_FORMAT_R16G16B16A16_SFLOAT
		126; // VK_FORMAT_D32_SFLOAT
	};
	return deref(arr # (t@s64));
};

impl_image_pixel_format_colour_aspect ::= func(r : gpu_resource -> s32)
{
	// assume the gpu_resource is a valid image or a window_resource
	if(r == (gpu_resource.window_resource))
	{
		// just get the swapchain colour format
		return swapchain.colour_format;
	}
	// not a window resource. its either a user-created resource or something invalid.
	if(impl_resource_is_real(r))
	{
		res ::= resource.data # (r@s64);
		return impl_image_type_pixel_format(res->info.image_type);
	}
	// should kinda panic here. return zero for now.
	return zero;
};

impl_image_pixel_format_depth_aspect ::= func(r : gpu_resource -> s32)
{
	// assume the gpu_resource is a valid image or a window_resource
	if(r == (gpu_resource.window_resource))
	{
		// just get the swapchain depth format
		return swapchain.depth_format;
	}
	// not a window resource. its either a user-created resource or something invalid.
	if(impl_resource_is_real(r))
	{
		res ::= resource.data # (r@s64);
		return impl_image_type_pixel_format(res->info.image_type);
	}
	// should kinda panic here. return zero for now.
	return zero;
};

impl_resource_is_real ::= func(r : gpu_resource -> bool)
{
	return (r != (gpu_resource.invalid)) && ((r != (gpu_resource.window_resource)));
};

impl_naive_image_size ::= func(i : gpu_resource_info -> u64)
{
	dims ::= i.image_dimensions;
	return impl_image_type_pixel_size(i.image_type) * deref(dims # 0)@_ * deref(dims # 1)@_;
};
